# 190905 Spark

#### 파이프 라인

- 머신러닝은 데이터 수집부터, 가공, 특성 추출, 알고리즘 적용 및 모델 생성, 평가, 배포 및 활용에 이르는 일련의 작업을 반복하며 수행된다.
- 파이프라인은 여러 종류의 알고리즘을 순차적으로 실행할 수 있게 지원하는 고차원 API이며, 파이프 라인 API를 이용해 머신러닝을 위한 워크 플로우를 생성할 수 있다.
- 파이프라인은 데이터 프레임을 사용한다.
- Transformer – org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임을 변형해 새로운 데이터프레임을 생성하는 용도로 사용
- Estimator - org.apache.spark.ml 패키지에 선언된 추상 클래스. 데이터프레임에 알고리즘을 적용해 새로운 트랜스포머를 생성하는 역할을 한다.
- Pipeline - org.apache.spark.ml 패키지에 선언된 클래스. 여러 알고리즘을 순차적으로 실행할 수 있는 워크플로우를 생성하는 평가자. 하나의 파이프라인은 여러 개의 파이프라인 스테이지(PipelineStage)로 구성되며, 등록된 파이프라인 스테이지들은 우선순위에 따라 순차적으로 실행된다.
- ParamMap : 평가자나 트랜스포머에 파라미터를 전달하기 위한 목적으로 사용되는 클래스
- Pipeline실습

```scala
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.ml.feature.VectorAssembler
//import org.apache.spark.sql.SparkSession
//object PipelineSample {  
//   def main(args: Array[String]) {    
//val spark = SparkSession.builder().appName("PipelineSample") .master("local[*]").getOrCreate()

// 훈련용 데이터 (키, 몸무게, 나이, 성별)
val training = spark.createDataFrame(Seq(      (161.0, 69.87, 29, 1.0),      (176.78, 74.35, 34, 1.0),      (159.23, 58.32, 29, 0.0))).toDF("height", "weight", "age", "gender") 
training.cache() 
 // 테스트용 데이터    
val test = spark.createDataFrame(Seq(      (169.4, 75.3, 42),      (185.1, 85.0, 37),      (161.6, 61.2, 28))).toDF("height", "weight", "age")    
training.show(false) 

//height, weight, age컬럼은 예측을 위한 특성으로 사용하고, gender는 label로 사용합니다.
//height, weight, age 특성만으로 구성된 벡터 생성
//VectorAssembler는 특성변환 알고리즘 클래스
val assembler = new VectorAssembler().setInputCols(Array("height", "weight", "age")).setOutputCol("features")    

// training 데이터에 features 컬럼 추가    
val assembled_training = assembler.transform(training)  
assembled_training.show(false)   

// 모델 생성 알고리즘 (로지스틱 회귀 평가자 - 이진분류 알고리즘)    
val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01).setLabelCol("gender") 

// 모델 생성    
val model = lr.fit(assembled_training) 

// 예측값 생성    
model.transform(assembled_training).show() 

// 파이프라인 (트랜스포머와 평가자를 하나의 파이프라인으로 묶어서 워크 플로우 생성)    
val pipeline = new Pipeline().setStages(Array(assembler, lr))    

// 파이프라인 모델 생성    
val pipelineModel = pipeline.fit(training) 

// 파이프라인 모델을 이용한 예측값 생성    
pipelineModel.transform(training).show()    
val path1 = "/output/sparkmllib/regression-model"    
val path2 = "/output/sparkmllib/pipelinemodel" 

// 모델 저장    
model.write.overwrite().save(path1)    
pipelineModel.write.overwrite().save(path2) 

// 저장된 모델 불러오기   
val loadedModel = LogisticRegressionModel.load(path1)    
val loadedPipelineModel = PipelineModel.load(path2) 

spark.stop  
}
}
```



자바 코드 예###########################

```java
import org.apache.spark.ml.Pipeline;
import org.apache.spark.ml.PipelineModel;
import org.apache.spark.ml.PipelineStage;
import org.apache.spark.ml.classification.LogisticRegression;
import org.apache.spark.ml.classification.LogisticRegressionModel;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import java.util.Arrays;
import java.util.List;
public class PipelineSample {
public static void main(String[] args) throws Exception {
    SparkSession spark = SparkSession.builder()
            .appName("PipelineSample")
            .master("local[*]")
 .getOrCreate();
   StructField sf1 = DataTypes.createStructField("height", DataTypes.DoubleType, true);
   StructField sf2 = DataTypes.createStructField("weight", DataTypes.DoubleType, true);
   StructField sf3 = DataTypes.createStructField("age", DataTypes.IntegerType, true);
   StructField sf4 = DataTypes.createStructField("label", DataTypes.DoubleType, true);
  StructType schema1 = DataTypes.createStructType(Arrays.asList(sf1, sf2, sf3, sf4));
    List<Row> rows1 = Arrays.asList(RowFactory.create(161.0, 69.87, 29, 1.0),
        RowFactory.create(176.78, 74.35, 34, 1.0),
            RowFactory.create(159.23, 58.32, 29, 0.0));
    // 훈련용 데이터 (키, 몸무게, 나이, 성별)
    Dataset<Row> training = spark.createDataFrame(rows1, schema1);
training.cache();
List<Row> rows2 = Arrays.asList(RowFactory.create(169.4, 75.3, 42),
            RowFactory.create(185.1, 85.0, 37),
            RowFactory.create(161.6, 61.2, 28));
    StructType schema2 = DataTypes.createStructType(Arrays.asList(sf1, sf2, sf3));
    // 테스트용 데이터
    Dataset<Row> test = spark.createDataFrame(rows2, schema2);
   training.show(false);
   VectorAssembler assembler = new VectorAssembler();
    assembler.setInputCols(new String[]{"height", "weight", "age"});
   assembler.setOutputCol("features");
   Dataset<Row> assembled_training = assembler.transform(training);
  assembled_training.show(false);
// 모델 생성 알고리즘 (로지스틱 회귀 평가자)
 LogisticRegression lr = new LogisticRegression();
 lr.setMaxIter(10).setRegParam(0.01);
 // 모델 생성
   LogisticRegressionModel model = lr.fit(assembled_training);
 // 예측값 생성
 model.transform(assembled_training).show();
    // 파이프라인
   Pipeline pipeline = new Pipeline();
    pipeline.setStages(new PipelineStage[]{assembler, lr});
    // 파이프라인 모델 생성
   PipelineModel pipelineModel = pipeline.fit(training); 
    // 파이프라인 모델을 이용한 예측값 생성
    pipelineModel.transform(training).show();
    String path1 = "/output/sparkmllib/regression-model" ;
    String path2 = "/output/sparkmllib/pipelinemodel";
    // 모델 저장
   model.write().overwrite().save(path1);
   pipelineModel.write().overwrite().save(path2);
    // 저장된 모델 불러오기
  LogisticRegressionModel loadedModel = LogisticRegressionModel.load(path1);
  PipelineModel loadedPipelineModel = PipelineModel.load(path2);
    spark.stop();
  }
}
```



- Python 코드 예

```python
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import LogisticRegressionModel
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.pipeline import Pipeline
from pyspark.ml.pipeline import PipelineModel
from pyspark.sql import SparkSession
spark = SparkSession
.builde
.appName("pipeline_sample")
.master("local[*]")
.getOrCreate()

# 훈련용 데이터 (키, 몸무게, 나이, 성별)

training = spark.createDataFrame([
  (161.0, 69.87, 29, 1.0),
 (176.78, 74.35, 34, 1.0),
(159.23, 58.32, 29, 0.0)]).toDF("height", "weight", "age", "gender")
training.cache()

# 테스트용 데이터

test = spark.createDataFrame([
(169.4, 75.3, 42),
 (185.1, 85.0, 37),
(161.6, 61.2, 28)]).toDF("height", "weight", "age")
training.show(truncate=False)
assembler = VectorAssembler(inputCols=["height", "weight", "age"], outputCol="features")

# training 데이터에 features 컬럼 추가

assembled_training = assembler.transform(training)
assembled_training.show(truncate=False)

# 모델 생성 알고리즘 (로지스틱 회귀 평가자)

lr = LogisticRegression(maxIter=10, regParam=0.01, labelCol="gender")

# 모델 생성

model = lr.fit(assembled_training)

# 예측값 생성

model.transform(assembled_training).show()

# 파이프라인

pipeline = Pipeline(stages=[assembler, lr])

# 파이프라인 모델 생성

pipelineModel = pipeline.fit(training)

# 파이프라인 모델을 이용한 예측값 생성

pipelineModel.transform(training).show()

  path1 = "/output/sparkmllib/regression-model" 
  path2 = "/output/sparkmllib/pipelinemodel" 

# 모델 저장

model.write().overwrite().save(path1)
pipelineModel.write().overwrite().save(path2)

# 저장된 모델 불러오기

loadedModel = LogisticRegressionModel.load(path1)
loadedPipelineModel = PipelineModel.load(path2)
spark.stop
```



#### 알고리즘

- Tokenizer – 공백 문자를 기준으로 입력 문자열을 개별 단어의 배열로 변환하고 이 배열을 값으로 하는 새로운 컬럼을 생성하는 트랜스포머. 문자열을 기반으로 하는 특성 처리에 자주 사용됨

- RegexTokenizer – 정규식을 사용하여 문자열을 기반으로 하는 특성 처리  
  - Tokenizer 실습

    ```scala
    import org.apache.spark.ml.feature.Tokenizer 
    
    val data = Seq("Tokenization is the process",
     "Refer to the Tokenizer").map(Tuple1(_))    
    val inputDF = spark.createDataFrame(data).toDF("input")    
    val tokenizer = new Tokenizer().setInputCol("input").setOutputCol("output")   
    val outputDF = tokenizer.transform(inputDF)    
    outputDF.printSchema()    
    outputDF.show(false)   
    ```

    

- TF-IDF(Term Frequency – Inverse Document Frequency) – 여러 문서 집합에서 특정 단어가 특정 문서 내에서 가지는 중요도를 수치화한 통계적 수치

- TF-IDF는 문서 내에서 단어의 출현 빈도를 나타내는 TF(단어 빈도)와 문서군 내에서 출현 빈도를 나타내는 IDF(문서 빈도, 빈도가 높을 수록 점수가 낮아짐)의 조합으로 결정되며, 문서 내에서 출현 빈도가 높은 단어일수록 높은 점수를 부여하되 특정 문서가 아닌 모든 문서에서 동일한 현상이 나타나면 흔하게 사용되는 중요하지 않은 단어로 간주해서 가중치를 낮춰주는 방법을 사용한다.

- 스파크 MLlib에서 TF-IDF 알고리즘은 TF 처리에 해당하는 부분은 트랜스포머 클래스로, IDF에 해당하는 부분은 평가자 클래스로 제공하고 있다.

- TF-IDF 실습

  ```scala
  import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}
  import org.apache.spark.sql.SparkSession
  
  object TfIDFSample {  
       def main(args: Array[String]) {    
       val spark = SparkSession .builder() .appName("TfIDFSample") .master("local[*]") .getOrCreate()   
       val df1 = spark.createDataFrame(Seq(      (0, "a a a b b c"),      
                                                              (0, "a b c"),     
                                                              (1, "a c a a d"))).toDF("label", "sentence")    
      val tokenizer = new Tokenizer().setInputCol("sentence").setOutputCol("words")    // 각 문장을 단어로 분리
      val df2 = tokenizer.transform(df1)    
      val hashingTF = new HashingTF().setInputCol("words").setOutputCol("TF-Features").setNumFeatures(20)   
      val df3 = hashingTF.transform(df2)    df3.cache()   
      val idf = new IDF().setInputCol("TF-Features").setOutputCol("Final-Features")   
      val idfModel = idf.fit(df3)   
      val rescaledData = idfModel.transform(df3)    
      rescaledData.select("words", "TF-Features", "Final-Features").show(false)    
     spark.stop  
       }
  }
  ```



- StringIndexer – 문자열 컬럼에 대응하는 숫자형 컬럼을 생성하는 평가자 

- StringIndexer는 문자열 레이블 컬럼에 적용하며 해당 컬럼의 모든 문자열에 노출 빈도에 따른 인덱스를 부여해서 숫자로 된 새로운 레이블 컬럼을 생성한다.

- StringIndexer는 트랜스포머가 아닌 평가자로서 fit() 메서드를 이용해 stringIndexerModel을 생성하며 이 모델을 이용해 문자열 인코딩을 수행할 수 있다. 

  ```scala
  import org.apache.spark.ml.feature.{IndexToString, StringIndexer} 
  val df1 = spark.createDataFrame(Seq((0, "red"),(1, "blue"),(2, "green"),
                                 (3, "yellow"))).toDF("id", "color")   
  val strignIndexer = new StringIndexer().setInputCol("color") .setOutputCol("colorIndex") .fit(df1)    
  val df2 = strignIndexer.transform(df1)    
    df2.show(false)  
  +---+------+----------+
  |id |color |colorIndex|
  +---+------+----------+
  |0  |red   |3.0       |
  |1  |blue  |1.0       |
  |2  |green |0.0       |
  |3  |yellow|2.0       |
  +---+------+----------+
  
  val indexToString = new IndexToString() .setInputCol("colorIndex") .setOutputCol("originalColor") 
  val df3 = indexToString.transform(df2)   
    df3.show(false)
  +---+------+----------+-------------+
  |id |color |colorIndex|originalColor|
  +---+------+----------+-------------+
  |0  |red   |3.0       |red          |
  |1  |blue  |1.0       |blue         |
  |2  |green |0.0       |green        |
  |3  |yellow|2.0       |yellow       |
  +---+------+----------+-------------+
  ```

  

#### 데이터 전처리 실습

##### # 1단계 : 데이터 전처리

> ##### MLlib 입력 데이터 형으로 변환하기 위해 DataFrame으로 생성 

- Schema 정의 : Case class 정의

  ```scala
  case class Weather( date: String,
                      day_of_week: String,
                      avg_temp: Double,
                      max_temp: Double,
                      min_temp: Double,
                      rainfall: Double,
                      daylight_hours: Double,
                      max_depth_snowfall: Double,
                      total_snowfall: Double,
                      solar_radiation: Double,
                      mean_wind_speed: Double,
                      max_wind_speed: Double,
                      max_instantaneous_wind_speed: Double,
                      avg_humidity: Double,
                      avg_cloud_cover: Double)
  case class Sales(date: String, sales: Double)
  ```



- 기상 데이터를 읽어 DataFrame으로 변환한다.

```scala
val weatherCSVRDD = sc.textFile("/data/sales/weather.csv")
val headerOfWeatherCSVRDD = sc.parallelize(Array(weatherCSVRDD.first))
val weatherCSVwithoutHeaderRDD = weatherCSVRDD.subtract(headerOfWeatherCSVRDD)
val weatherDF = weatherCSVwithoutHeaderRDD.map(_.split(",")).
      map(p => Weather(p(0),
      p(1),
      p(2).trim.toDouble,
      p(3).trim.toDouble,
      p(4).trim.toDouble,
      p(5).trim.toDouble,
      p(6).trim.toDouble,
      p(7).trim.toDouble,
      p(8).trim.toDouble,
      p(9).trim.toDouble,
      p(10).trim.toDouble,
      p(11).trim.toDouble,
      p(12).trim.toDouble,
      p(13).trim.toDouble,
      p(14).trim.toDouble
    )).toDF()

// 매출 데이터를 읽어 DataFrame으로 변환한다
val salesCSVRDD = sc.textFile("/data/sales/sales.csv")
val headerOfSalesCSVRDD = sc.parallelize(Array(salesCSVRDD.first))
val salesCSVwithoutHeaderRDD = salesCSVRDD.subtract(headerOfSalesCSVRDD)
val salesDF = salesCSVwithoutHeaderRDD.map(_.split(",")).map(p => Sales(p(0), p(1).trim.toDouble)).toDF()

//정의된 스키마 확인
println(weatherDF.printSchema)  
root
 |-- date: string (nullable = true)
 |-- day_of_week: string (nullable = true)
 |-- avg_temp: double (nullable = false)
 |-- max_temp: double (nullable = false)
 |-- min_temp: double (nullable = false)
 |-- rainfall: double (nullable = false)
 |-- daylight_hours: double (nullable = false)
 |-- max_depth_snowfall: double (nullable = false)
 |-- total_snowfall: double (nullable = false)
 |-- solar_radiation: double (nullable = false)
 |-- mean_wind_speed: double (nullable = false)
 |-- max_wind_speed: double (nullable = false)
 |-- max_instantaneous_wind_speed: double (nullable = false)
 |-- avg_humidity: double (nullable = false)
 |-- avg_cloud_cover: double (nullable = false)

()

println(salesDF.printSchema)
root
 |-- date: string (nullable = true)
 |-- sales: double (nullable = false)

()
```



- Date칼럼을 이용해 두 데이터를 결합한다.

```scala
scala> val salesAndWeatherDF = salesDF.join(weatherDF, "date")

//결과 확인
scala> salesAndWeatherDF.show
+----------+--------+-----------+--------+--------+--------+--------+--------------+------------------+--------------+---------------+---------------+--------------+----------------------------+------------+---------------+
|      date|   sales|day_of_week|avg_temp|max_temp|min_temp|rainfall|daylight_hours|max_depth_snowfall|total_snowfall|solar_radiation|mean_wind_speed|max_wind_speed|max_instantaneous_wind_speed|avg_humidity|avg_cloud_cover|
+----------+--------+-----------+--------+--------+--------+--------+--------------+------------------+--------------+---------------+---------------+--------------+----------------------------+------------+---------------+
| 2014/9/29|441000.0|         월|    20.0|    23.6|    18.2|     1.5|           0.5|               0.0|           0.0|            0.0|            1.4|           4.5|                         7.1|        88.9|            8.5|
| 2014/9/30|546000.0|         화|    14.5|    21.3|    12.7|     6.5|           0.0|               0.0|           0.0|            0.0|            2.8|           9.1|                        14.5|        94.3|           10.0|
| 2014/2/28|414000.0|         금|     5.0|     6.5|     3.4|     0.0|           0.0|               3.5|           0.0|            0.0|            2.5|           4.9|                        10.1|        73.3|            9.8|
|2014/11/13|454400.0|         목|     2.1|     5.1|    -1.9|     0.0|           9.3|               0.0|           0.0|            0.0|            4.2|           8.9|                        14.8|        24.6|            0.0|
|  2014/1/4|508700.0|         토|     1.2|     3.8|     0.0|     7.4|           0.0|               6.0|           6.0|            0.0|            1.8|           6.0|                         8.4|        87.4|           10.0|
| 2014/2/24|423900.0|         월|     3.7|     7.2|    -1.8|     0.0|           7.3|              19.0|           0.0|            0.0|            1.2|           2.5|                         5.5|        79.8|            0.3|
| 2014/9/16|498800.0|         화|    19.5|    23.0|    16.5|     0.0|           7.7|               0.0|           0.0|            0.0|            2.0|           5.1|                         7.8|        79.3|            6.4|
.
.
(중략)

+----------+--------+-----------+--------+--------+--------+--------+--------------+------------------+--------------+---------------+---------------+--------------+----------------------------+------------+---------------+
only showing top 20 rows
```



- 주말인지 아닌지를 기준으로 삼아 원래 데이터를 변환

```scala
val isWeekend = udf((t: String) => if(t.contains("일") || t.contains("토")) 1d 
                                   else 0d)
val replacedSalesAndWeatherDF = salesAndWeatherDF.withColumn("weekend", isWeekend(salesAndWeatherDF("day_of_week"))).drop("day_of_week")
```



- 독립변수를 취사선택
  - 상관관계가 강한 변수들이 회귀분석용 공식의 입력 변수로 함께 쓰이면 계수에 잘못된 영향을 끼쳐 정확성이 기대에 못 미치는 모델이 만들어질 수 있기 때문에

```scala
scala> val selectedDataDF = replacedSalesAndWeatherDF.select (
       "sales",
       "avg_temp",
       "rainfall",
       "weekend")
```



- DataFrame의 map 메소드를 이용해 DataFrame의 모든 row에 LabeledPoint를 생성해 요소로 갖도록 한다.

```scala
scala> val labeledPoints = selectedDataDF.rdd.map { row =>
     LabeledPoint(row.getDouble(0),
     			  Vectors.dense(
     				row.getDouble(1),
     				row.getDouble(2),
     				row.getDouble(3))) }
```



- 입력 데이터 표준화하기 (평균 0, 분산 1인 스케일러 사용, 평균값을 조정하고 스케일링을 개별적으로 유효화 또는 무효화를 할 수 있다.)
  - 독립 변수마다 척도와 분산이 달라서 회귀 알고리즘에 데이터를 그대로 입력하면 일정하지 않은 척도나 분산의 영향을 받은 결과가 나오기 때문에 이를 피하기 위해 MLLib가 제공하는 표준화 기능을 이용

```scala
scala> val scaler = new StandardScaler(withMean = true, withStd = true).fit(
       labeledPoints.map(x=>x.features))

scala> val scaledLabeledPoints = labeledPoints.map { x =>
       LabeledPoint(x.label, scaler.transform(x.features)) }
```



##### # 2단계 : 알고리즘 적용

- 반복횟수의 최댓값을 지정하고 모델을 생성하는 API를 호출한다.

```scala
scala> import org.apache.spark.mllib.regression.LinearRegressionWithSGD

scala> val numIter = 20

scala> scaledLabeledPoints.cache

scala> val linearRegressionModel =
       LinearRegressionWithSGD.train(scaledLabeledPoints, numIter)
```



- 예측하고 싶은 데이터를 벡터 형식으로 작성하고 생성된 linearRegressionModel에 입력해 매출을 예측

```scala
//두 개의 데이터 정의
scala> val targetDataVector1 = Vectors.dense(15.0, 15.4, 1)

scala> val targetDataVector1 = Vectors.dense(20.0, 0, 0)

//이 값들을 표준화용 스케일러에 입력해 표준화된 벡터를 얻는다.
scala> val targetScaledDataVector1 = scaler.transform(targetDataVector1)
targetScaledDataVector1: org.apache.spark.mllib.linalg.Vector = [0.190280969076364,1.1261669278047137,1.5759200651384164]

scala> val targetScaledDataVector2 = scaler.transform(targetDataVector2)
targetScaledDataVector2: org.apache.spark.mllib.linalg.Vector = [0.7466849627506041,-0.2771361705444259,-0.6328018794378197]

//linearRegressionModel의 predict메소드를 이용해 예상 매출을 계산한다.
scala> linearRegressionModel.predict(targetScaledDataVector1)
res17: Double = 108474.02249443227

scala> linearRegressionModel.predict(targetScaledDataVector2)
res18: Double = -47173.92459941932

//정량적으로 결과를 해석하고자 생성된 모델의 계수를 확인한다.
scala> linearRegressionModel.weights
res19: org.apache.spark.mllib.linalg.Vector = [-3757.0766186795554,-2995.2122553924905,71426.23166500554]

//주말의 계수가 크고 기온과 강수량 계수는 음수 => 기온이 높고 비가 내리면 고객이 줄어드는 경향!
```



##### # 예측 모델의 평가 : 예측값을 어느 정도 신뢰할수 있는지에 대한 평가

> 정답 데이터가 첨부된 원래 데이터를 무작위로 2개로 분할해 평가하는 홀드아웃을 적용해보자. '학습 데이터'를 이용해 모델을 생성하고 '평가 데이터'를 모델에 적용해 예측값과 정답을 비교함으로써 예측 모델을 평가해보자.

- 전처리를 끝낸 데이터를 60%의 학습 데이터와 40%의 평가 데이터로 무작위로 나눈다. (RDD의 randomSplit(arr, seed) 메소드를 이용, 첫 번째 인수에는 분할 비율을, 분할에 쓰이는 시드값을 고정하고 싶다면 두 번째 인수에 고정 시드값을 지정)

```scala
scala> val splitScaledLabeledPoints = scaledLabeledPoints.randomSplit(Array(0.6, 0.4), seed = 11L)

scala> val trainingScaledLabeledPoints = splitScaledLabeledPoints(0).cache()

scala> val testScaledLabeledPoints = splitScaledLabeledPoints(1)
```



- LinearRegressionWithSGD의 train메소드에 입력 데이터를 건네주고 모델을 생성한다.

```scala
scala> val linearRegressionModel2 = LinearRegressionWithSGD.train(
       trainingScaledLabeledPoints, numIter)
```



- 평가용 데이터를 모델에 입력하고 (예측값, 실제값)의 형식으로 예측 결과와 정답 데이터를 작성한다.

```scala
scala> val scoreAndLabels = testScaledLabeledPoints.map { point =>
       val score = linearRegressionModel2.predict(point.features)
       (score, point.label) }
```



- 회귀용 평가 라이브러리를 임포트하고 예측 결과와 정답 데이터를 입력해 매트릭스를 계산한다.
  - RegressionMetrics를 이용하면 평균 절댓값 오차, 평균 제곱 오차, 제곱근 평균 제곱 오차 등을 계산할 수 있다.

```scala
scala> import org.apache.spark.mllib.evaluation.RegressionMetrics

scala> val metrics = new RegressionMetrics(scoreAndLabels)

scala> metrics.rootMeanSquaredError
res20: Double = 589274.8489941772 
```



##### # 예측 모델의 보존

- HDFS의 디렉토리에 매개변수 저장하고 load메소드로 매개변수 이용

```scala
scala> linearRegressionModel.save(sc, "/output/mllib/model/")

scala> import org.apache.spark.mllib.regression.LinearRegressionModel

scala> val sameLinearRegressionModel =
       LinearRegressionModel.load(sc, "/output/mllib/model/")
```