# Regression

## I. 회귀분석이란?

### 1. 들어가기

- 회귀분석

  - 회귀란 말은 어딘가로 돌아간다는 의미
  - 도대체 어디로 돌아가길래 이런 이름이 붙었을까?

- 회귀분석의 목적

  - 주어진 (독립)변수로 (종속)변수를 예측하기 위해서!
  - 단순회귀
    - 독립변수 1개 & 종속변수 1개
  - 다중회귀
    - 독립변수 2개이상 & 종속변수 1개

- 회귀분석의 예

  - 수입(독립변수)과 지출(종속변수)의 관계를 알아보자 (수입이 있기 때문에 지출이 있기 때문에 이렇게 설정하자. 원인(수입)과 결과(지출))

  ![to1](2.%20regression.assets/to1.JPG)

  - 분석의 목적 : 만약 수입이 1억이라면 지출은 얼마일지 알아보기

    => 회귀분석을 통해 수입이 1억인 사람의 지출을 예측해야 한다.

    => 이를 예측하기 위해 필요한 것 : "추세선"

  ![to2](2.%20regression.assets/to2.JPG)

  

### 2. 추세선과 회귀분석

데이터(점)들을 가장 잘 설명해줄 수 있는 어떠한 선(즉, 추세선)을 찾는 것 => 이것이 회귀분석의 목적이다!

추세선과 1억이 만나는 점, 즉 그 때의 지출이 예측값이 될 것이기 때문!

=> 어떻게 추세선을 합리적인 방법으로 구해낼 수 있을까?

- 추세선

  ![to3](2.%20regression.assets/to3.JPG)

  - 이 때 y는 종속변수, x는 독립변수로 우리가 가진 데이터들을 통해 알 수 있는 값이다. 즉, 회귀분석은 아직 모르는 a와 b 값을 구하는 과정!
  - a는 y절편(x가 0일 때의 y값), b는 그래프의 기울기(y의 증가량/x의 증가량) 

- 그렇다면 점들은 어떻게 표현할 수 있을까?

  ![to4](2.%20regression.assets/to4.JPG)

  - e는 오차 (추세선과 점들 사이의 오차)
  - i는 개별 점들의 값



![to5](2.%20regression.assets/to5.JPG)

이 추세선이 가장 합리적으로 데이터들을 설명하는 선이라고 해도 그 사이에는 오차가 발생하기 마련이다. 

이 때 오차(잔차) = 측정값 - 예측값

![to6](2.%20regression.assets/to6.JPG)

가장 합리적인 추세선은 이 오차가 가장 작은 추세선이겠다. 그런데 오차에 +와 -가 혼재한다. 

=> 오차의 제곱이 최소화된 추세선. 더 정확하게는 오차의 제곱합이 최소화된 추세선이 가장 합리적인 추세선! (분산의 개념과 매우 유사)

이 오차의 제곱이 최소화된 추세선을 어떻게 구할 것이냐? A : 최소제곱법!

- 최소제곱법 : 오차의 제곱의 합을 최소로 만드는 방법
  - 최소제곱법으로 구해진 직선이 우리가 원하는 회귀분석의 식
    - 이 직선은 평균을 지난다. (평균으로의 회귀!!)
    - 이 방법을 영어로 Ordinary Least Square(OLS)라고 한다.



------

## II. 결과표 해석하기

### 1. 변수 & 가설

- 사용할 데이터와 변수

  - House sales price in Kings County, USA(Kaggle.com)
  - 종속변수 : price( =매매가격)
  - 독립변수 : sqft_living(=거실 크기)

- 연구가설 : 거실의 크기가 클수록 매매 가격이 비쌀 것이다.

  - 논리 : 거실의 크기가 크다는 것은 집의 규모가 크다는 의미일 것이므로 매매 가격이 더 비싸질 것이지 않을까? 라는 논리

  

### 2. 결과표 해석

![to7](2.%20regression.assets/to7-1569397869296.JPG)

- 해석을 어떻게 해야할까?
  - Intercept (절편/상수)는 우리의 관심사가 아니다.  Why? 애초에 독립변수인 거실크기가 관심사니까
  - 관심사는 sqft_living(거실크기)의 Estimate(회귀계수)의 방향과 p-value임



- 회귀계수

![to8](2.%20regression.assets/to8.JPG)

​		=> y = a + bx의 b가 바로 회귀 계수이다. 이 회귀직선의 식은 <u>y = -43581 + 281x</u> 가 되겠다.

​		=> 여기서 281은 x가 1 증가할 때, y가 281 증가한다는 뜻이다. 

- SE는 표준오차, t는 t값, p는 유의확률인데, 왜 이 결과들을 알아야 하는지는 이후에 살펴보기로 하자.

- 결론적으로, 이 결과표를 해석하면 거실크기가 1만큼 증가할 때, 매매가격은 281$ 증가하며, 이 결과는 우리의 연구가설 (거실의 크기가 클수록 매매 가격이 비쌀 것이다)을 지지해준다는 해석이 된다. 즉, 두 변수(독립변수와 종속변수)의 관계가 유의하다고 해석할 수 있겠다.



-----

## III. 표준오차

### 1. 들어가기

![to9](2.%20regression.assets/to9.JPG)

만약에 이렇게 두 데이터를 가지고 회귀분석을 했는데 그 결과 회귀방정식이 y = 0.5 + 2.4x로 같은 식이 나왔다고 해보자. 이렇게 됐을 때, 우리는 이 두 데이터를 보고 똑같은 회귀방정식을 적용할 수 있다고 생각할 수 있을까?

- x가 1 증가할 때, y는 2.4 증가한다는 의미가 됨
- 그런데 다른점이 있다면, 왼쪽의 데이터는 직선을 중심으로 잘 모여있고, 우측은 직선을 중심으로 퍼져있음

- 똑같은 의미를 가진다고 할 수 있을까?

최소제곱법은 오차의 제곱합이 최소가 되는 회귀방정식을 구해줄 뿐, 이 회귀식의 회귀계수가 우연인지 아닌지(즉, 유의한지 아닌지)까지 알려주지는 않는다.



통계의 기본 질문부터 시작해보면, 회귀방정식의 결과를 봤을 때, x가 1 증가할 때 y가 2.4 증가한다고 하는데, 이 2.4의 증가가 우연히 발생하지는 않았을까? 라는 생각으로 시작할 수 있다. 즉, 우리는 회귀계수 2.4가 우연인지 아닌지 판단해야 하는데, 어떻게 판단해야 할까? 역시 비교대상이 필요하다. 이 비교대상이 표준오차인 것!



### 2. 표준오차란?

잔차들의 분산

우리는 뭘 하든지 대부분 모집단이 아닌 표본으로 통계분석을 한다. 이 때, 우리는 우리가 가진 <u>표본이 얼마나 모집단에 가까운지를 판단</u>해야 한다. 이를 위해, 모집단의 평균을 아직은 모르는데 어쨌든 모집단의 평균을 평균의 참값이라고 하고, 먼저 <u>표본집단의 평균이 모집단의 평균과 얼마나 가까운지를 계산해 볼 필요</u>가 있다. 이론적으로는 같은 모집단에서 적합한 방법으로 표본을 구해도 표본집단의 평균은 매번 조금씩 다를 수 밖에 없다. 표본평균들의 표준편차(즉, 각 표본평균들이 모평균에서 떨어진 정도)가 표준오차인데, 어쨌든 결론적으로 표준오차가 작으면 참값에 더 가깝다는 것이고, 표준오차가 크면 참값에서 더 멀다는 뜻, 즉, 작을수록 모집단을 더 잘 대변할 수 있다는 것!



표준오차가 작다는 것은 회귀계수 2.4가 우연일 가능성이 낮다는 것, 반대로 표준오차가 크다는 것은 회귀계수 2.4가 우연일 가능성이 높다는 것! 더 나아가면, 아마도 2.4라는 값은 유의할 것이다, 유의하지 않을 것이다로 예상할 수 있는 것! 다른 말로 p-value < 0.05 /  p-value > 0.05라고도 예측할 수 있을 것!

![to10](2.%20regression.assets/to10.JPG)

- 회귀계수는 최소제곱법으로 구해진다.
  - 그러나 그렇게 계산된 회귀계수가 우연인지 아닌지는 모른다.
  - 그래서 이 회귀계수가 우연일 확률을 알기 위해 표준오차를 사용한다.

- 표준오차가 작으면 회귀계수가 우연일 확률이 낮다.
  - 표준오차가 작다는 것은 데이터가 회귀직선에 가까이 몰려있다는 의미
- 표준오차가 크면 회귀계수가 우연일 확률이 크다.
  - 표준오차가 크다는 것은 데이터가 회귀직선에서 멀리 퍼져있다는 의미
- 그렇다면 이 확률을 어떻게 계산할까?
  - t-test로 계산한다!



-----

## IV. t-test

![to11](2.%20regression.assets/to11.JPG)

- t-test 계산법
  - t-value = 기울기(즉, 회귀계수) / 표준오차(기울기의 표준오차)
  - df(자유도) = 1
  - 구한 값이 해당되는 유의확률을 확인하면 됨



- 왜 t-test일까?

![to12](2.%20regression.assets/to12.JPG)

집단1, 집단2가 있고 집단1,2의 평균이 각각 그림과 같다고 할 때, 이 때 t-test 계산법은 t-value = (집단1의 평균 - 집단2의 평균) / 표준편차 이겠다.



![to13](2.%20regression.assets/to13.JPG)

그런데, 여기서 편의상 집단1과 집단2를 더미변수 0, 1로 각각 두면(더미변수는 이후 알아보자), 위 식과 같이 결국 t-test의 계산법과 같아진다. but, 앞에서는 두 개의 집단의 평균이 같은지 다른지, 그냥 평균값을 가지고 t-test를 했다면, 회귀분석에서는 같은 t-test인데 실제 이 기울기가 0인지 아닌지를 test하게 된다. 

그런데, 왜 기울기가 0인지 아닌지냐?

![to14](2.%20regression.assets/to14.JPG)

기울기 0이 의미하는 것은?

- 독립변수가 증가해도 종속변수는 아무 변화가 없음
- 그러므로 유의하지 않다
- 왜냐하면 독립변수가 원인으로서 아무 역할도 못하므로
- 따라서 이 경우 표준오차가 아무리 작아도 의미가 없는 회귀방정식이 되겠다.



![to15](2.%20regression.assets/to15.JPG)

위와 같은 곡선그래프나 원그래프역시 직선그래프의 방정식으로는 결국 기울기가 0이 나온다. 따라서 우리는 회귀분석을 하기 앞서서 산포도를 먼저 알아볼 필요가 있다.



- 회귀분석의 특징
  - 만약 데이터가 곡선 형태, 원(구)형태로 돼 있다면 기울기는 0이다. 즉, 직선의 형태로 돼 있지 않다면 분석할 수 없다.
  - 회귀분석 전에 산포도를 찍어봐야 한다.
    - 직선형태의 데이터 분포가 나타나지 않을 경우 다른 방법을 찾아야 한다. 왜냐하면 결국 회귀분석이란 y = a + bx의 직선을 구하는 것이기 때문에!
  - 회귀계수(기울기)는 결국 t-test의 평균값 차이와 동일한 개념이다.
    - 따라서 회귀계수는 t-test로 유의성을 테스트한다.



- 회귀계수 t-test의 통계적 가설
  - 귀무가설 : 기울기는 0이다.
  - 대립가설 : 기울기는 0이 아닐 것이다.



- 아래와 같은 경우 회귀계수는 얼마라고 해야 할까?

![to16](2.%20regression.assets/to16.JPG)

 : 실질적으로 우연히 나온 값이므로 0이라고 보는 것!



----

## V. 결정계수(R-Squared)

회귀분석이란? 종속변수의 분산을 독립변수로 설명하는 과정

=> 통계적 분석이란? 이론/논리를 통해서 종속변수를 설명할 수 있는 모델을 만들어(이 모델에 들어가는 독립변수를 설정한 후 ) 종속변수의 분산을 모델(즉, 독립변수)로 설명

여기서 우리의 모델(즉, 독립변수)이 큰 문제가 없다면 우리의 모델로 설명하고 남은 오차는 랜덤한 오차이다.

![to17](2.%20regression.assets/to17.JPG)

이 때, 결정계수는 종속변수의 전체분산 대비 설명된 분산이다. 



![to18](2.%20regression.assets/to18.JPG)





결정계수 계산법

![to19](2.%20regression.assets/to19.JPG)

- SST (Sum of Squared Total) : (각 데이터의 종속변수 값 - 종속변수의 평균값)^

- SSE (Sum of Squared Error) : (각 데이터 값 - 회귀계수)^



- 결정계수는 0부터 1까지만 존재
  - 결정계수=0 : 모델 설명력이 0
  - 결정계수=1 : 모델 설명력이 100%



결정계수가 의미하는 것은 무엇이고 어떻게 해석해야 하나?

- 결정계수는 모델의 분산 설명력이라고 볼 수 있다.
- 이는 우리가 만든 모델(즉, 독립변수)이 얼마나 데이터를 잘 설명했는지를 의미한다.



그렇다면 결정계수가 높으면 무조건 좋은 것인가?

- 절대 그렇지 않다.
- 나름의 의미는 있으나, 높은 결정계수가 모든 것을 완벽하게 하지는 못한다.
  - 결정계수를 확인하기 전에 잔차도(residual plot)가 랜덤하게 분포함을 확인해야 한다.
  - 의미 없는 독립변수의 추가조차도 결정계수를 약간이라도 증가시킨다.
  - 그러나 독립변수의 추가는 자유도를 1 증가시켜 비용이 발생하게 된다.
  - 높은 결정계수는 또한 과적합(overfitting)문제로부터 자유롭지 않다.



잔차도란?

앞에서 우리는 종속변수의 분산을 모델(즉, 독립변수)로 설명하는데, 여기서 모델이 큰 문제가 없는 경우 모델로 설명하고 남은 오차가 잔차도이다.

회귀분석의 중요한 전제는 모델로 설명하고 남은 이 잔차도가 랜덤한 오차여야 한다는 것이다.



![to20](2.%20regression.assets/to20.JPG)

위와 같은 경우는 잔차에 규칙성이 없지만, 

![to21](2.%20regression.assets/to21.JPG)

위와 같은 경우는 잔차가 점점 커지는 경향을 보인다. 이런 경우 아무리 결정계수가 높아도 그 결정계수는 애초에 모델이 잘못됐다는 의미가 되기 때문에 의미가 없어진다. 



그렇다면 결정계수 대신 사용할 수 있는 것은?

앞서 이야기 했듯이 결정계수의 단점은 독립변수가 무한대로 증가하면 어차피 결정계수가 증가하는데 이 때 독립변수가 증가하는 만큼 자유도를 손실한다는  부분인데, 이에 대한 보정이 필요하다.

여기서 보정이란 추가된 독립변수가 자유도 1을 잃고도 충분히 분산을 설명했는지의 여부를 의미하며 자유도가 감안된 결정계수가 필요한데, 이것이 adj.Rsquare 즉, 수정된 결정계수이다. 보통 결정계수와 함께 adj.R-square를 같이 리포트해서 둘의 크기가 심하게 다르면 의미 없는 독립변수를 너무 많이 넣었다는 의미로 진단한다.



 

과적합

우리는 거의 표본만을 대상으로 분석하는데, 만약 모델이 이번에 수집한 표본에서만 높은 결정계수를 보인다면..?

이것은 단 한번 우연히 이 표본에만 과하게 적합할 뿐, 다시 표본을 수집하면 절대 높은 결정계수를 확인할 수 없다는 것을 의미한다. 또한 이는 우리의 모델이 이 표본에만 우수함을 의미한다. 따라서 우리의 모델은 큰 의미가 없다는 것을 의미하게 되겠다.



과적합 판단 및 해결책

- Cross-validation : 표본을 랜덤하게 둘로 나누어 한 표본에서 모델을 구축하고 난 뒤 다른 표본에서 모델의 적합성을 다시 테스트 (빅데이터 덕분에 표본의 양이 많아서 요즘은 아주 합리적인 방법)



결국, 결정계수보다 중요한 것은, 모델에 사용된 독립변수의 논리성과 이론적 근거이다!



-----

## VI. 표준화계수

표준화계수란? 종속변수에 대한 독립변수들의 단위(scaling)를 통일시킨 계수

표준화계수의 단위는 없어져 모든 독립변수를 같은 단위 상에서 비교 가능하다



표준화계수의 특징

- 원점(0, 0)을 지나가는 회귀직선

- 절편(intercept)이 0
- 회귀계수의 크기 비교가 가능



아예 모든 독립변수를 표준화시킨 후에 푀귀분석을 해도 된다.



표준화계수 계산법

![to22](2.%20regression.assets/to22.JPG)





그래서 무슨 뜻일까?

![to23](2.%20regression.assets/to23.JPG)

위와 같은 경우, 세 독립변수 중 sqft_living(거실의 크기)가 표준화계수가 가장 크므로, 세 독립변수 중 가장 영향력이 크다.

문제점 : 세 독립변수의 표준화계수로 크기를 비교할 수는 있으나 이 표준화계수의 차이가 우연히 발생한 것인지의 추가적인 테스트가 필요하다.



해석방법 : 비표준화계수의 경우, 거실사이즈가 1커지면 주택가격이 271$ 오른다라고 해석함. 표준화계수의 경우 거실 사이즈가 1 표준편차만큼 커지면 주택가격이 1표준편차*0.6773$ 오른다.  말이 복잡해서 설명은 비표준화계수로 한다!



왜 표준화계수를 쓰는가?

1) 논리적으로 (0, 0)을 회귀직선이 지나가야만 하는 경우!

가령, 경제학 데이터에서 수입과 지출의 관계를 분석하면 대부분 영점을 지나가지 않는다. Why? 우리가 수집한 데이터에는 수입이 0인 사람이 없기 때문이다. 그렇다고 내버려둘 수는 없으므로 강제로 영점을 지나가게 한다. (표준화계수의 y절편은 0이다! 즉, 영점을 지난다!) 그 방법이 표준화계수의 사용이다. 



2) 여러 개의 독립변수들 중 가장 영향력이 큰 변수를 찾기 위해

상대적 중요도를 알고싶어서 사용! (방법은 매우 복잡)

이 때 주의할 점은 어떤 독립변수의 표준화계수가 가장 클 수는 있는데, 이것이 통계적으로 가장 큰 것인지는 추가적인 테스트를 해야 한다. 따라서, 해석할 때 주의해야 한다. 특히, 표준화계수가 비슷할 경우, 약간의 크고 작음은 통계적 유의성이 없을 수 있다. 너무 단정적으로 해석하지 말 것!



------

## VII. 다중공선성

![to24](2.%20regression.assets/to24.JPG)

색칠한 부분이 다중공선성 문제 : 즉, 독립변수들끼리 너무 많이 겹쳐서 발생하는 문제!

다중공선성이란

- 상관관계가 매우 높은 독립변수들이 동시에 모델에 포함될 때 발생
- 왜 문제가 되는가?
- 완벽한 다중공선성이 있다면,
  - 만약 두 변수가 완벽하게 다중공선성에 걸려 있다면, 같은 변수를 두 번 넣은 것이나 다름없다 => 최소제곱법 계산상 어려워진다.
- 완벽한 다중공선성이 아니어도 문제인가?
  - 그렇다. 다중공선성이 높아지면 회귀계수의 표준오차가 비정상적으로 커진다.
  - 왜 문제인가?
    - 회귀계수의 유의성은 t-value에 의해 결정된다.
    - t-value는 회귀계수/표준오차
    - 결국, 다중공선성으로 인해 표준오차가 비정상적으로 커지면 t-value가 작아져서 유의해야 할 변수가 유의하지 않게 된다. 



다중공선성을 어떻게 찾아낼까?

- 산포도를 찍고 상관계수를 본다.
  -  두 독립변수의 산포도를 봤을 때 심각하게 상관관계가 높아 보이면,
  - 상관계수를 확인 => 상관계수가 만약 가령 0.9를 넘는다면 다중공선성의 문제가 있을 수 있다.
- 공차(tolerance)를 확인
  - 공차란 한 개의 독립변수를 종속변수로, 나머지 독립변수를 독립변수로 해서 새로운 회귀분석을 했을 때 나오는 결정계수를 이용해 (1-결정계수)로 계산한 값
  - 만약 공차가 1이라면 독립변수 간에 심각한 상관관계가 있음을 의미한다.
    - 이 경우 공차는 0이 될 것이고,
    - 따라서 공차가 0이면 완벽한 상관성을 의미해 다중공선성이 심각함을 의미한다.
- 분산팽창지수(VIF : Variance Inflation Factor)
  - 회귀계수의 표준오차(즉, Variance=분산)가 팽창된 정도
  - VIF = 1 / 공차 = 1 / (1-결정계수)
  - VIF가 크다는 것은 다중공선성이 크다는 의미
  - 일반적으로 10보다 크면 문제가 있다고 판단한다. (but, 이는 연속형변수에 해당되는 경우)
  - 더미변수의 경우 좀 더 낮게 기준이 잡히는데 VIF가 3 이상이면 다중공선성 문제가 있다고 판단
- 상태지수 (Condition Index)
  - 흔하게 사용되지는 않음
  - 100 이상이면 심각한 다중공선성이 존재



다중공선성이 발생하면 어떻게 해결해야 하나?

1) 다중공선성이 큰 변수가 유의한지 아닌지 확인해야 한다

- 다중공선성이 있음에도 불구하고 해당 독립변수가 유의하다면, 이것은 표준오차가 비정상적으로 팽창돼 있음에도 유의하다는 의미이므로, 그 자체로 매우 유의하다는 의미가 돼 그대로 두어도 무방하다

2) 해당 변수를 제거한다

- 가장 일반적인 방법
- 문제는 해당변수가 연구의 중심이 되는 중요변수일 경우 문제가 된다.
- 이러한 경우 근본적인 원인은 연구에 대한 논리적 구성이 사전에 부족했기 때문

3) 주성분분석으로 변수를 재조합

- 겹치는 분산을 제거하는 효과가 있다
- but, 제거된 분산이 꼭 겹치는 것만은 아니어서 경우에 따라 재조합된 변수들이 이상한 결과를 내는 경우가 있다

4) 다중공선성이 발생한 독립변수들을 합친다

- 이 경우 두 변수의 상관관계가 높다는 것을 의미하기 때문에 합칠 수 있지만, 유의하다고 해도 해석이 어려워진다. 

5) 능형 회귀분석

6) Mean Centering 방법

- 모든 변수를 각 변수의 평균값으로 뺀 뒤에 회귀분석을 하는 방법
- 어느정도의 효과는 있으나 완벽하지 않다.