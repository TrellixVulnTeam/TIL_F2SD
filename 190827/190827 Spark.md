# 190827 Spark

#설치 및 실행

1. https://spark.apache.org/downloads.html에서 spark-2.4.3-bin-hadoop2.7.tgz  다운로드

2. ```
   [hadoop@master Downloads]$ su -
   [root@master ~]# cd /usr/local/
   
   #압축 풀기
   [root@master local]# tar zxvf /home/hadoop/Downloads/spark-2.4.3-bin-hadoop2.7.tgz
   
   #권한명 변경
   [root@master local]# ln -s  spark-2.4.3-bin-hadoop2.7  spark
   [root@master local]# ls -l
   [root@master local]# chown -R hadoop:hadoop spark
   [root@master local]# ls -l
   ```

3. ```
   [root@master ~]# su - hadoop
   [hadoop@master ~]$ vi .bash_profile
   
   #다음과 같이 추가
   export SPARK_HOME=/usr/local/spark
   export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
   export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop
   
   #PATH는 다음과 같이 변경
   export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin
   
   #변경 후에는 로그아웃 한번 했다가 로그인하기
   ```

4. ```
   [hadoop@master ~]$ spark-shell --master local verbose
   ```



#실습하기 (Word Count)

```scala
//로컬 파일시스템에서 파일을 읽여들여서 RDD로 생성
scala> val file = sc.textFile("file:///usr/local/spark/README.md")
file: org.apache.spark.rdd.RDD[String] = file:///usr/local/spark/README.md MapPartitionsRDD[1] at textFile at <console>:24

//RDD로부터 한 행(라인) 단위로 처리 - 단어 분리 후 새로운 RDD 생성 후 저장
scala> val words = file.flatMap(_.split(" "))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25

//같은 단어끼리 모아서 요약(개수) 계산 - map 형태로 단어와 반복 횟수
scala> val result = words.countByValue
result: scala.collection.Map[String,Long] = Map(site, -> 1, Please -> 4, Contributing -> 1, GraphX -> 1, project. -> 1, "" -> 72, for -> 12, find -> 1, Apache -> 1, package -> 1, Hadoop, -> 2, review -> 1, Once -> 1, For -> 3, name -> 1, this -> 1, protocols -> 1, Hive -> 2, in -> 6, "local[N]" -> 1, MASTER=spark://host:7077 -> 1, have -> 1, your -> 1, are -> 1, is -> 7, HDFS -> 1, Data. -> 1, built -> 1, thread, -> 1, examples -> 2, developing -> 1, using -> 5, system -> 1, than -> 1, Shell -> 2, mesos:// -> 1, 3"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3). -> 1, easiest -> 1, This -> 2, -T -> 1, [Apache -> 1, N -> 1, integration -> 1, <class> -> 1, different -> 1, "local" -> 1, README -> 1, YARN"](http://spark.apache.org/docs/latest/building-spark.h...

scala> result.get("For").get
res0: Long = 3
```



#sbt 설치하기

1. https://www.scala-sbt.org/download.html에서 sbt-1.2.7.tgz 파일 다운로드

2. ```
   [hadoop@master ~]$ su -
   
   #압축 풀기
   [root@master ~]# tar zxvf /home/hadoop/Downloads/sbt-1.2.7.tgz -C /opt/
   
   #압축이 잘 풀렸나 확인
   [root@master ~]# ls -l /opt/
   
   [root@master ~]# su - hadoop
   [hadoop@master ~]$ vi .bash_profile
   
   #bash_profile을 다음과 같이 추가 및 수정
   export SBT_HOME=/opt/sbt
   export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME/bin:$SBT_H
   OME/bin
   ```

3. ```
   [hadoop@master ~]$ sbt about
   ```



#sbt 실습

```
#스칼라 어플리케이션 프로젝트 폴더 생성
hadoop@master ~]$ mkdir spark-simple-app
[hadoop@master ~]$ cd spark-simple-app

#소스코드파일 저장디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala

#SBT 설정파일 저장디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir 

#소스코드 저장될 패키지 디렉토리 생성
[hadoop@master spark-simple-app]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master spark-simple-app]$ cd src/main/scala/lab/spark/example
[hadoop@master example]$ vi SundayCount.scala
```



- SundayCount.scala

  ```
  package lab.spark.example
  
  import org.joda.time.{DateTime, DateTimeConstants}
  import org.joda.time.format.DateTimeFormat
  import org.apache.spark.{SparkConf, SparkContext}
  
  object SundayCount {
  
    def main(args: Array[String]) {
      if (args.length < 1) {
        throw new IllegalArgumentException(
          "명령 인수에 날짜가 기록된 파일의 경로를 지정해 주세요.")
      }
  
    val filePath = args(0)
    val conf = new SparkConf
    val sc = new SparkContext(conf)
  
    try {
      val textRDD = sc.textFile(filePath)
  
      val dateTimeRDD = textRDD.map { dateStr =>
        val pattern =
          DateTimeFormat.forPattern("yyyyMMdd")
        DateTime.parse(dateStr, pattern)
      }
  
      val sundayRDD = dateTimeRDD.filter { dateTime =>
        dateTime.getDayOfWeek == DateTimeConstants.SUNDAY
      }
  
      val numOfSunday = sundayRDD.count
        println(s"주어진 데이터에는 일요일이 ${numOfSunday}개 들어 있습니다.")
      } finally {
          sc.stop()
      }
    }
  }
  
  ```

```
[hadoop@master example]$ cd ~/spark-simple-app
[hadoop@master spark-simple-app]$ vi build.sbt
```



- build.sbt

  ```
  name := "spark-simple-app"
  version := "0.1"
  scalaVersion := "2.11.12"
  libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided", "joda-time" % "joda-time" % "2.8.2")
  assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)
  ```

```
[hadoop@master project]$ vi plugins.sbt
#아래와 같이 설정
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")

#jar파일 생성
[hadoop@master project]$ cd ~/spark-simple-app/
[hadoop@master spark-simple-app]$ sbt assembly
[success] Total time: 5 s, completed Aug 27, 2019 10:29:24 PM

#어플리케이션 빌드 성공 후 결과 프로젝트 루트 디렉토리(~/spark-simple-app) 밑에 target 디렉토리 아래 jar파일 생성된 것 확인
```



```
#하둡 시작
[hadoop@master ~]$ cd /usr/local/hadoop-2.7.7/sbin
[hadoop@master sbin]$ ./start-all.sh

#하둡 파일 시스템에 date.txt파일 업로드
[hadoop@master ~]$ hadoop fs -mkdir  /data/spark/
[hadoop@master ~]$ hadoop fs -put date.txt  /data/spark/
 
 #Word Count 실행
[hadoop@master ~]$ spark-submit --master local 
--class lab.spark.example.SundayCount 
--name SundayCount  
~/spark-simple-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar  
/data/spark/date.txt

#결과
19/08/27 22:55:56 INFO scheduler.DAGScheduler: Job 0 finished: count at SundayCount.scala:32, took 0.499699 s
주어진 데이터에는 일요일이 6개 들어 있습니다.
19/08/27 22:55:56 INFO server.AbstractConnector: Stopped Spark@1a6f5124{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/08/27 22:55:56 INFO ui.SparkUI: Stopped Spark web UI at http://master:4040
19/08/27 22:55:56 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/08/27 22:55:56 INFO memory.MemoryStore: MemoryStore cleared
19/08/27 22:55:56 INFO storage.BlockManager: BlockManager stopped
19/08/27 22:55:56 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
19/08/27 22:55:56 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/08/27 22:55:56 INFO spark.SparkContext: Successfully stopped SparkContext
```



#Spark Context

- 애플리케이션과 스파크 클러스터와의 연결을 담당하는 객체
- 모든 스파크 애플리케이션은 SparkContext를 이용해 RDD나 acumulator 또는 broadcast 변수 등을 다루게 된다.
- 스파크 애플리케이션을 수행하는 데 필요한 각종 설정 정보를 담는 역할을 한다.



#RDD 생성

- 스파크에서 사용하는 기본 분산 데이터 모델
- 하나의 파일과 같은 외부 데이터 소스로부터 생성하는 방법
- 기존의 RDD로부터 또 다른 RDD를 생성하는 방법



#스파크 클러스터

- 클러스터는 여러 대의 서버가 마치 한 개의 서버처럼 동작하는 것
- 스파크는 클러스터 환경에서 동작하며, 대량의 데이터를 여러 서버로 나누어 병렬로 처리한다.



#분산 데이터로서 RDD

- Resilient Distributed DatabaseSets 회복력을 가진 분산 데이터 집합
- 데이터를 처리하는 과정에서 집합을 이루고 있던 데이터의 일부에 문제가 생겨도 스스로 알아서 복구할 수 있다.
- 다수의 데이터 요소가 모인 집합
- 불변성



#파티션

- RDD 데이터는 클러스터를 구성하는 여러 서버에 나누어 저장되는데 스파크는 분할된 데이터를 파티션이라는 단위로 관리한다.



#Job과 Executor

- 스파크 프로그램을 실행하는 것을 Job을 실행한다고 하고 각 서버마다 Executor라는 프로세스가 생성돼 각자 할당된 파티션을 처리



#Driver Program

- 스파크에서 Job을 실행하는 프로그램



#Transformation

- RDD의 형태로 변형하는 연산
- 기존의 RDD를 바꾸는 것이 아니며 새로운 RDD를 하나 더 생성



#Action

- RDD가 아닌 다른 타입의 결과를 반환하는 연산
- RDD의 각 요소를 이용해 어떤 결과값을 얻어내는 연산



#스파크 컨텍스트 생성

- 스파크 애플리케이션과 클러스터의 연결을 관리하는 객체로서 모든 스파크 애플리케이션은 반드시 스파크 컨텍스를 생성해야 한다.
- 스파크 컨텍스트를 생성할 때는 스파크 동작에 필요한 여러 설정 정보를 지정할 수 있다.



#RDD 생성

1. 드라이버 프로그램의 컬렉션객체(시퀀스 타입 객체)를 이용해서 RDD 생성
2. 파일이나 데이터베이스 같은 외부 데이터를 이용해 RDD 생성



#Collect

- RDD의 모든 원소를 모아서 배열로 돌려준다.
- 파일이나 데이터베이스 같은 외부 데이터를 이용해 RDD 생성
- RDD에 있는 모든 요소들이 collect 연산을 호출한 서버의 메모리에 수집된다.
- 전체 데이터를 모두 담을 수 있을 정도의 충분한 메모리 공간이 확보돼 있는 상태에서만 사용해야 한다.

```scala
[hadoop@master ~]$ spark-shell

scala> val rdd = sc.parallelize(1 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> val result = rdd.collect
result: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> println(result.mkString(", "))
1, 2, 3, 4, 5, 6, 7, 8, 9, 10
```



#count

- RDD를 구성하는 전체 요소의 개수를 반환

```scala
scala> val result2 = rdd.count
result2: Long = 10

scala> println(result2)
10
```



#Transformation

- Map 연산 : 요소 간의 맵핑을 정의한 함수를 RDD에 속하는 모든 요소에 적용해 새로운 RDD를 생성

  - 하나의 입력을 받아 하나의 값을 반환하는 함수를 인자로 받는다.
  - RDD에 속하는 모든 요소에 적용한 뒤 그 결과로 구성된 새로운 RDD를 생성해서 반환
  - 인자로 사용하는 함수의 반환 타입에 제약이 없다.

  ```scala
  scala> val rdd = sc.parallelize(1 to 5)
  rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24
  
  scala> val result = rdd.map(_ + 1)
  result: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at map at <console>:25
  
  scala> println(result.collect.mkString(", "))
  2, 3, 4, 5, 6
  ```

- 그룹화 연산 : 특정 조건에 따라 요소를 그룹화하거나 특정 함수를 적용

- 집합 연산

- 파티션 연산

- 필터와 정렬 연산



#flatmap

- TraversableOnce는 스칼라에서 사용하는 이터레이터 타입 중 하나이다.

```scala
scala> val fruits = List("apple, orange", "grape, apple mango", "blueberry, tomato, orange")
fruits: List[String] = List(apple, orange, grape, apple mango, blueberry, tomato, orange)

scala> val rdd1 = sc.parallelize(fruits)
rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:26

scala> val rdd2 = rdd1.flatMap(_.split(","))
rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:25

scala> print(rdd2)
MapPartitionsRDD[4] at flatMap at <console>:25
scala> print(rdd2.collect.mkString(", "))
apple,  orange, grape,  apple mango, blueberry,  tomato,  orange
```

```scala
scala> val rdd1 = sc.parallelize( 1 to 10, 3)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at <console>:24

scala> val rdd2 = rdd1.mapPartitions(numbers => {
     |    print("DB 연결 !!!")
     |    numbers.map {
     |       number => number +1
     |    }
     | })
rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at mapPartitions at <console>:25

scala> println(rdd2.collect.mkString(", "))
DB 연결 !!!DB 연결 !!!DB 연결 !!!2, 3, 4, 5, 6, 7, 8, 9, 10, 11
```



#mapValues

```scala
scala> val rdd = sc.parallelize(List("a", "b", "c")).map((_,1))
rdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[9] at map at <console>:24

scala> val result = rdd.mapValues(i => i+1)
result: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at mapValues at <console>:25

scala> println(result.collect.mkString("\t"))
(a,2)	(b,2)	(c,2)
```



#zip

```scala
scala> val rdd1 = sc.parallelize(List("a", "b", "c"))
rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[11] at parallelize at <console>:24

scala> val rdd2 = sc.parallelize(List(1, 2, 3))
rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at <console>:24

scala> val result = rdd1.zip(rdd2)
result: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[13] at zip at <console>:27

scala> println(result.collect.mkString(", "))
(a,1), (b,2), (c,3)
```



#groupBy

- RDD의 요소를 일정한 기준에 따라 여러 개의 그룹으로 나누고 이 그룹으로 구성된 새로운 RDD를 생성

```scala
scala> val rdd = sc.parallelize(1 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:24

scala> val result = rdd.groupBy {
       case i : Int if (i % 2 == 0) => "even"
       case _ => "odd"
       }
result: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[16] at groupBy at <console>:25

scala> result.collect.foreach {
       v => println(s"${v._1}, [${v._2.mkString(", ")}]")
       }
even, [2, 4, 6, 8, 10]
odd, [1, 3, 5, 7, 9]
```





#실습하기

```
#스파크 어플리케이션 프로젝트 폴더 생성
[hadoop@master ~]$ mkdir wordcount-app

[hadoop@master ~]$ cd wordcount-app

# 소스 코드 파일 저장 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala  
#sbt 설정 파일 저장  디렉토리 생성
[hadoop@master ~]$ mkdir project

# 소스 코드 저장될 패키지 디렉토리 생성
[hadoop@master ~]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master ~]$ cd  src/main/scala/lab/spark/example
[hadoop@master ~]$ vi WordCount.scala


[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ vi build.sbt

name := "spark-simple-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)


[hadoop@master ~]$ cd project
[hadoop@master ~]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")

#어플리케이션 빌드
[hadoop@master ~]$ cd ~/wordcount-app
[hadoop@master ~]$ sbt assembly

#데이터 소스 생성
[hadoop@master ~]$ vi simple-words.txt
cat
dog
.org
cat
rabbit
bear
cat
&&
tiger
dog
rabbit
100
bear
tiger
cat
rabbit
?bear

#하둡 파일 시스템에 simple-words.txt파일 업로드
[hadoop@master ~]$ hadoop fs -mkdir  /data/spark/
[hadoop@master ~]$ hadoop fs -put simple-words.txt  /data/spark/
 
[hadoop@master ~]$ spark-submit --master local 
--class lab.spark.example.WordCount 
--name WordCount  
~/wordcount-app/target/scala-2.11/wordcount-app-assembly-0.1.jar  
/data/spark/simple-words.txt
```



#cogroup

```scala
scala> val rdd = sc.parallelize( List(("k1", "v1"), ("k2", "v2"), ("k1", "v3") ))
rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[3] at parallelize at <console>:24

scala> val rdd2 = sc.parallelize( List(("k1", "v4")))
rdd2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[4] at parallelize at <console>:24

scala> val result = rdd.cogroup(rdd2)
result: org.apache.spark.rdd.RDD[(String, (Iterable[String], Iterable[String]))] = MapPartitionsRDD[6] at cogroup at <console>:27

scala> result.collect.foreach {
     |    case (k, (v_1, v_2)) => {
     |         println(s"($k, [${v_1.mkString(",")}], [${v_2.mkString(", ")}])")
     |    }
     | }
(k1, [v1,v3], [v4])
(k2, [v2], [])
```



#distinct()

- RDD의 원소에서 중복을 제외한 요소로만 구성된 새로운 RDD를 생성하는 메소드

```scala
scala> val rdd = sc.parallelize(List(1, 2, 3, 1, 2, 3, 1, 2, 3))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at <console>:24

scala> val result = rdd.distinct()
result: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[10] at distinct at <console>:25

scala> println(result.collect.mkString(", "))
2, 1, 3
```



#cartesian()

- 두 RDD 요소의 카테시안 곱을 구하고 그 결과를 요소로 하는 새로운 RDD를 생성



#subtract()

- rdd1.subtract(rdd2)는 rdd1에 속하고 rdd2에는 속하지 않는 요소로 구성된 새로운 RDD를 생성하는 메소드

#intersection()

- 교집합

```scala
ala> val rdd1 = sc.parallelize( List("a", "b", "c", "d", "e"))
rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[13] at parallelize at <console>:24

scala> val rdd2 = sc.parallelize( List("d", "e"))
rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[14] at parallelize at <console>:24

scala> val result = rdd1.subtract(rdd2)
result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[18] at subtract at <console>:27

scala> println(result.collect.mkString(", "))
b, a, c

scala> val result = rdd1.intersection(rdd2)
result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[24] at intersection at <console>:27

scala> println(result.collect.mkString(", "))
d, e
```



#reduceByKey()

- RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메소드
- 같은 키를 가진 값들을 하나로 병합해 키 + 값 쌍으로 구성된 새로운 RDD를 생성한다.



#combineByKey()

- 평균구하기 예제

```scala
scala> case class Record(var amount: Long, var number: Long=1) {
       def map(v: Long) = Record(v)
       def add(amount: Long): Record = {
       add(map(amount))
       }
       def add(other: Record) : Record = {
       this.number += other.number
       this.amount += other.amount
       this
       }
       override def toString: String = s"avg:${amount / number}"
       }
defined class Record

scala> val data = Seq(("Math", 100L), ("Eng", 80L), ("Math", 50L), ("Eng", 60L), ("Eng", 90L))
data: Seq[(String, Long)] = List((Math,100), (Eng,80), (Math,50), (Eng,60), (Eng,90))

scala> val rdd = sc.parallelize(data)
rdd: org.apache.spark.rdd.RDD[(String, Long)] = ParallelCollectionRDD[27] at parallelize at <console>:26

scala> val createCombiner = (v:Long) => Record(v)
createCombiner: Long => Record = <function1>

scala> val mergeValue = (c:Record, v:Long) => c.add(v)
mergeValue: (Record, Long) => Record = <function2>

scala> val mergeCombiners = (c1:Record, c2:Record) => c1.add(c2)
mergeCombiners: (Record, Record) => Record = <function2>

scala> val result = rdd.combineByKey(createCombiner, mergeValue, mergeCombiners)
result: org.apache.spark.rdd.RDD[(String, Record)] = ShuffledRDD[28] at combineByKey at <console>:31

scala> println(result.collect.mkString("\n"))
(Math,avg:75)
(Eng,avg:76)
```

