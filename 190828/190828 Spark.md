# 190828 Spark

## Reveiw

**#Spark란? **

- 인메모리 기반의 대용량 데이터 고속 처리 엔진으로 범용 분산 클러스터 컴퓨팅 프레임워크



**#Spark 구성 요소**

- 클러스터 매니저 : Spark standalone, Yarn, Mesos
- SparkCore 
- Spark SQL
- Spark Streaming - 실시간 처리
- MLlib
- Graph X



**#데이터 처리하기 추상화된 모델** : RDD(복구가능한 분산 데이터셋)



**#SparkApplication 구현 단계**

1. SparkContext 생성
   - Spark애플리케이션과 Spark 클러스터와의 연결을 담당하는 객체
   - 모든 스파크 애플리케이션은 SparkContext를 이용해 RDD나 accumulator 또는 broadcast 변수 등을 다루게 됩니다.
   - Spark 애플리케이션을 수행하는 데 필요한 각종 설정 정보를 담는 역할을 한다
2. RDD (불변데이터 모델, parition가능) 생성
   collection, HDFS, hive , CSV 등..
3. RDD 처리 - 변환연산(RDD의 요소의 구조 변경, filter처리, grouping...)    
4. 집계, 요약 처리 - Action연산 
5. 영속화 



**#SparkApplication** => Job



**#Spark 클러스터 환경에서 node들**  : SparkClient, Master노드, Worker노드

- SparkClient 역할 - SparkApplication 배포하고 실행을 요청

- Spark Master노드 역할 -  Spark 클러스터 환경에서 사용가능한 리소스들의 관리
- Spark Worker노드 역할 - 할당받은 리소스(CPU core, memory)를 사용해서  SparkApplication 실행 

- Spark Worker노드에서 실행되는 프로세스 - Executor는 RDD의 partition을 task단위로 실행



**#Spark의 장점**

1. 반복처리와 연속으로 이루어지는 변환처리를 고속화(메모리 기반)
2. 딥러닝, 머신러닝 등의 실행환경에 적합한 환경 제공
3. 서로 다른 실행환경과 구조, 데이터들의 처리에 대해서 통합 환경 제공



**#Spark 함수**

- sc.textFile() : file로부터 RDD 생성
- collect
- map, flatMap()
- mkString("구분자") : 배열의 요소를 문자열로 결합하기 위해 사용





----------------------------

## 오늘자 Spark

**#집계와 관련된 연산들**

- **aggregateBykey()**

  - RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메서드
  - 병합을 시작할 초기값을 생성, combineByKey()와 동일한 동작을 수행

  ```
  def  aggregateByKey[U](zeroValue: U)(seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, C)]
  ```

  ```scala
  var data = Seq(("Math", 100L),("Eng", 80L), ("Math", 50L), ("Eng", 70L), ("Eng", 90L))
  val rdd = sc.parallelize(data)
  //초기값
  val zero = Record(0, 0)
  val mergeValue = (c:Record, v: Long) => c.add(v)
  val mergeCombiners = (c1:Record, c2:Record) => c1.add(c2)
  //병합을 위한 초기값을 전달함!!
  val result = rdd.aggregateByKey(zero)(mergeValue, mergeCombiners)
  println(result.collect.mkString(", \t"))
  ```

  

- **pipe()** : pipe를 이용하면 데이터를 처리하는 과정에서 외부 프로세스를 활용할 수 있다.

  ```scala
  //세 개의 숫자로 구성된 문자열을 리눅스의 cut 유틸리티를 이용해 분리한 뒤 첫 번째와 세 번째 숫자를 뽑아내는 예제
  val rdd = sc.parallelize(List("1, 2, 3", "4, 5, 6", "7, 8, 9"))
  val result = rdd.pipe("cut -f 1, 3 -d,")
  println(result.collect.mkString(", "))
  ```

  

- **coalesce(), repartition()**

  - 현재의 RDD의 파티션 개수를 조정할 수 있습니다.
  - 파티션의 크기를 나타내는 정수를 인자로 받아서 파티션의 수를 조정
  - repartition()이 파티션 수를 늘리거나 줄이는 것을 모두 가능
  - coalesce()는 줄이는 것만 가능
  - repartition()은 셔플을 기반으로 동작
  - coalesce()은 강제로 셔플을 수행하라는 옵션을 지정하지 않는한 셔플을 사용하지 않음
  - 데이터 필터링 등의 작업으로 데이터 수가 줄어들어 파티션의 수를 줄이고자 할 때는 상대적으로 성능이 좋은 coalesce()를 사용하고, 파티션 수를 늘여야 하는 경우에만 repartition()를 사용하는 것이 좋다.

  ```scala
  val rdd1 = sc.parallelize( 1 to 1000000, 10)
  val rdd2 = rdd1.coalesce(5)
  val rdd3 = rdd2.repartition(10) 
  println(s"partition size: ${rdd1.getNumPartitions}")
  println(s"partition size: ${rdd2.getNumPartitions}")
  println(s"partition size: ${rdd3.getNumPartitions}")
  ```

  

- **repartitionAndSortWithinPartitions()**

  - RDD를 구성하는 모든 데이터를 특정 기준에 따라 여러 개의 파티션으로 분리하고 각 파티션 단위로 정렬을 수행한 뒤 이 결과로 새로운 RDD를 생성해 주는 메서드
  - 데이터가 키와 값 쌍으로 구성돼 있어야 하고 메서드를 실행할 때 각 데이터가 어떤 파티션에 속할지 결정하기 위한  파티셔너(Partitioner)를 설정해야 한다.
  - 파티셔너는 각 데이터의 키 값을 이용해 데이터가 속할 파티션을 결정하게 되는데, 이때 키 값을 이용한 정렬도 함께 수행된다.
  - 파티션 재할당을 위해 셔플을 수행하는 단계에서 정렬도 함께 다루게 되어 파티션과 정렬을 각각 따로 따로 하는 것에 비해 더 높은 성능을 발휘할 수 있다.
  - foreachPartition()  RDD메서드

  ```scala
  val r = scala.util.Random
  //1부터 100까지의 정수 중에 난수 10개를 생성
  val data = for (i <- 1 to 10) yield (r.nextInt(100), "-")
  val rdd1 = sc.parallelize(data)
  
  //Hash 방식으로 파티션의 결과를 균등하게 가짐
  val rdd2 = rdd1.repartitionAndSortWithinPartitions(new HashPartitioner(3))
  
  //결과 검증
  rdd2.foreachPartition(it => { println("========"); it.foreach(v=>println(v)) })
  ```



- **partitionBy()**

  - •RDD의 구성요소가 키와 값의 쌍으로 구성된 경우에 사용할 수 있는 메서드

    •org.apache.spark.Partitioner 클래스의 인스턴스를 인자로 전달해야 합니다.

    •Partitioner는 각 요소의 키를 특정 파티션에 할당하는 역할을 수행

    •스파크에서 기본적으로 제공하는 것은 HashPartitioner, RangePartitioner

    •RangePartitioner는 순서가 있는 요소들(Sortable)로 구성된 RDD에 사용할 수 있으며, 각 요소를 목표 파티션에 크기에 맞게 일정 크기의 구간으로 나누는 방식을 사용

    •HashPartitioner 각 요소의 키 값으로부터 해시값을 취해 이 값을 기준으로 파티션을 결정하는 방법



- **sort실습**

```
scala> val textRDD = sc.textFile("/data/spark/README.md")
textRDD: org.apache.spark.rdd.RDD[String] = /data/spark/README.md MapPartitionsRDD[1] at textFile at <console>:24

scala> val wordCandidateRDD = textRDD.flatMap(_.split("[ ,.]"))
wordCandidateRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25

scala> val wordRDD = wordCandidateRDD.filter(_.matches("""\p{Alnum}+"""))
wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at filter at <console>:25

scala> val wordAndOnePairRDD = wordRDD.map((_, 1))
wordAndOnePairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at <console>:25

scala> val wordAndCountRDD = wordAndOnePairRDD.reduceByKey(_ + _)
wordAndCountRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at <console>:25

scala> val countAndWordRDD = wordAndCountRDD.map { wordAndCount => (wordAndCount._2, wordAndCount._1) }
countAndWordRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[6] at map at <console>:25

scala> val sortedCWRDD = countAndWordRDD.sortByKey(false)
sortedCWRDD: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[9] at sortByKey at <console>:25

scala> val sortedWCRDD = sortedCWRDD.map { countAndWord => (countAndWord._2, countAndWord._1) }
sortedWCRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at map at <console>:25

scala> val sortedWCArray = sortedWCRDD.collect
sortedWCArray: Array[(String, Int)] = Array((the,24), (Spark,17), (to,17), (for,12), (apache,10), (and,10), (a,9), (can,7), (is,7), (run,7), (on,7), (in,6), (using,5), (also,5), (of,5), (Hadoop,5), (Python,4), (documentation,4), (build,4), (Please,4), (with,4), (if,4), (including,4), (an,4), (You,4), (you,4), (see,4), (package,3), (general,3), (locally,3), (example,3), (how,3), (Scala,3), (one,3), (cluster,3), (For,3), (use,3), (or,3), (programs,3), (This,2), (Hive,2), (SparkPi,2), (refer,2), (Interactive,2), (thread,2), (detailed,2), (return,2), (built,2), (Shell,2), (class,2), (sc,2), (building,2), (set,2), (guidance,2), (SQL,2), (supports,2), (particular,2), (following,2), (which,2), (should,2), (To,2), (be,2), (do,2), (It,2), (Maven,2), (tests,2), (examples,2), (at,2), (command,2), ...
scala> sortedWCArray.foreach(println)
(the,24)
(Spark,17)
(to,17)
(for,12)
(apache,10)
(and,10)
(a,9)
(can,7)
(is,7)
(run,7)
(on,7)
(in,6)
(using,5)
(also,5)
(of,5)
(Hadoop,5)
(Python,4)
.
.
.
(directory,1)
(Pi,1)
(protocols,1)
(name,1)
(available,1)
(core,1)
(more,1)
(tools,1)
(option,1)
(must,1)
(system,1)
```







**#WordCountTop3 실습**

```
#스칼라 어플리케이션 프로젝트 폴더 생성
hadoop@master ~]$ mkdir top3-simple-app
[hadoop@master ~]$ cd top3-simple-app

#소스코드파일 저장디렉토리 생성
[hadoop@master top3-simple-app]$ mkdir -p src/main/scala

#SBT 설정파일 저장디렉토리 생성
[hadoop@master top3-simple-app]$ mkdir project

#소스코드 저장될 패키지 디렉토리 생성
[hadoop@master top3-simple-app]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master top3-simple-app]$ cd src/main/scala/lab/spark/example
[hadoop@master example]$ vi WordCountTop3.scala
```



- WordCountTop3.scala

  ```
  package lab.spark.example
  
  import org.apache.spark.{SparkConf, SparkContext}
  
  object WordCountTop3 {
  
    def main(args: Array[String]) {
      require(args.length >= 1,
        "드라이버 프로그램의 인수에 단어를 세고자 하는 " +
        "파일의 경로를 지정해 주세요.")
  
      val conf = new SparkConf
      val sc = new SparkContext(conf)
  
      try {
        val filePath = args(0)
        val wordAndCountRDD = sc.textFile(filePath)
                                .flatMap(_.split("[ ,.]"))
                                .filter(_.matches("""\p{Alnum}+"""))
                                .map((_, 1))
                                .reduceByKey(_ + _)
  
      val top3Words = wordAndCountRDD.map {
        case (word, count) => (count, word)
      }.sortByKey(false).map {
        case (count, word) => (word, count)
      }.take(3)
        top3Words.foreach(println)
      } finally {
        sc.stop()
      }
    }
  }
  ```

```
[hadoop@master example]$ cd ~/top3-simple-app
[hadoop@master top3-simple-app]$ vi build.sbt
```



- build.sbt

  ```
  name := "top3-simple-app"
  version := "0.1"
  scalaVersion := "2.11.12"
  libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided", "joda-time" % "joda-time" % "2.8.2")
  assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)
  ```

```
[hadoop@master project]$ vi plugins.sbt
#아래와 같이 설정
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")

#jar파일 생성
[hadoop@master project]$ cd ~/top3-simple-app/
[hadoop@master spark-simple-app]$ sbt assembly

#어플리케이션 빌드 성공 후 결과 프로젝트 루트 디렉토리(~/top3-simple-app) 밑에 target 디렉토리 아래 jar파일 생성된 것 확인
```



```
 #Word Count 실행
[hadoop@master ~]$ spark-submit --master local 
--class lab.spark.example.WordCountTop3 
--name WodCountTop3  
~/top3-simple-app/target/scala-2.11/top3-simple-app-assembly-0.1.jar  
/data/spark/README.md

#결과
19/08/28 19:35:38 INFO scheduler.DAGScheduler: ResultStage 2 (take at WordCountTop3.scala:27) finished in 0.117 s
19/08/28 19:35:38 INFO scheduler.DAGScheduler: Job 0 finished: take at WordCountTop3.scala:27, took 1.102200 s
(the,24)
(Spark,17)
(to,17)
```





**#csv파일 실습**

- csv파일들

```
products.csv==============================
0,송편(6개),12000
1,가래떡(3개),16000
2,연양갱,5000
3,호박엿(6개),16000
4,전병(20장),4000
5,별사탕,3200
6,백설기,3500
7,약과(5개),8300
8,강정(10개),15000
9,시루떡,6500
10,무지개떡,4300
11,깨강정(5개),14000
12,수정과(6컵),19000
13,절편(10개),15000
14,팥떡(8개),20000
15,생과자(10개),17000
16,식혜(2캔),21000
17,약식,4000
18,수수팥떡(6개),28000
19,팥죽(4개),16000
20,인절미(4개),10000

 
  

sales-october.csv===============================

5830,2014-10-02 10:20:38,16,28
5831,2014-10-02 15:13:04,15,22
5832,2014-10-02 15:21:53,2,10
5833,2014-10-02 16:22:05,18,13
5834,2014-10-06 12:04:28,19,18
5835,2014-10-06 12:54:13,10,18
5836,2014-10-06 15:43:54,1,8
5837,2014-10-06 17:33:19,10,22
5838,2014-10-11 10:28:00,20,19
5839,2014-10-11 15:00:32,15,3
5840,2014-10-11 15:06:04,15,14
5841,2014-10-11 15:45:38,18,1
5842,2014-10-11 16:12:56,4,5
5843,2014-10-13 10:13:53,3,12
5844,2014-10-13 15:02:23,15,19
5845,2014-10-13 15:12:08,6,6
5846,2014-10-13 17:17:20,10,9
5847,2014-10-18 11:08:11,15,22
5848,2014-10-18 12:01:47,3,8
5849,2014-10-18 14:25:25,6,10
5850,2014-10-18 15:18:50,10,16
5851,2014-10-20 13:06:00,11,21
5852,2014-10-20 16:07:04,13,29
5853,2014-10-20 17:29:24,5,4
5854,2014-10-20 17:47:39,8,17
5855,2014-10-23 10:02:10,2,24
5836,2014-10-23 11:22:53,8,19
5857,2014-10-23 12:29:16,7,7
5858,2014-10-23 14:01:56,12,26
5859,2014-10-23 16:09:39,8,13
5860,2014-10-23 17:26:46,8,19




sales-november.csv====================================
5861,2014-11-01 10:47:52,15,22
5863,2014-11-01 11:44:54,8,26
5864,2014-11-01 14:29:51,18,10
5865,2014-11-01 17:50:00,6,17
5867,2014-11-04 10:03:57,15,16
5868,2014-11-04 11:22:55,15,13
5869,2014-11-04 16:32:09,19,6
5870,2014-11-10 11:12:30,17,27
5871,2014-11-10 13:32:53,17,13
5872,2014-11-10 15:31:21,4,15
5873,2014-11-10 16:03:01,6,5
5874,2014-11-10 17:52:20,12,28
5875,2014-11-15 11:36:39,3,5
5876,2014-11-15 14:08:26,9,7
5877,2014-11-15 15:05:21,10,0
5878,2014-11-18 11:17:09,7,16
5879,2014-11-18 14:50:37,9,3
5880,2014-11-18 16:23:39,4,20
5881,2014-11-18 17:28:31,18,25
5882,2014-11-22 10:50:24,7,26
5883,2014-11-22 11:43:31,3,3
5884,2014-11-22 12:57:22,4,12
5885,2014-11-22 15:20:17,19,25
5886,2014-11-25 16:42:07,10,27
5887,2014-11-25 17:38:03,14,0
5888,2014-11-25 18:30:36,10,8
5889,2014-11-25 18:41:57,11,10
5890,2014-11-30 14:30:08,11,17
5862,2014-11-30 14:57:47,8,22
5866,2014-11-30 15:17:29,8,24
```



- CSV파일로부터 RDD생성하기

```scala
scala> def createSalesRDD(csvFile: String) = {
       val logRDD = sc.textFile(csvFile)
       logRDD.map { record =>
       val splitRecord = record.split(",")
       val productId = splitRecord(2)
       val numOfSold = splitRecord(3).toInt
       (productId, numOfSold)
       }
       }
createSalesRDD: (csvFile: String)org.apache.spark.rdd.RDD[(String, Int)]

scala> val salesOctRDD = createSalesRDD("/data/spark/sales-october.csv")
salesOctRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[2] at map at <console>:26

scala> val salesNovRDD = createSalesRDD("/data/spark/sales-november.csv")
salesNovRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[5] at map at <console>:26
```



- filter메소드를 호출해 10월과 11월에 50개 이상 팔린 상품 찾기

```scala
scala> import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD

scala> def createOver50SoldRDD(rdd: RDD[(String, Int)]) = {
       rdd.reduceByKey(_ + _).filter(_._2 >= 50)
       }
createOver50SoldRDD: (rdd: org.apache.spark.rdd.RDD[(String, Int)])org.apache.spark.rdd.RDD[(String, Int)]

scala> val octOver50SoldRDD = createOver50SoldRDD(salesOctRDD)
octOver50SoldRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[7] at filter at <console>:25

scala> val novOver50SoldRDD = createOver50SoldRDD(salesNovRDD)
novOver50SoldRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[9] at filter at <console>:25
```



- 두 개의 RDD를 키 기준으로 결합하기

```scala
scala> val bothOver50SoldRDD = octOver50SoldRDD.join(novOver50SoldRDD)
bothOver50SoldRDD: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[12] at join at <console>:28

scala> bothOver50SoldRDD.collect.foreach(println)
(8,(68,72))
(15,(80,51))
```



- map메소드를 사용해 각 상품별로 10월과 11월의 판매량을 더하기

```scala
scala> val over50SoldAndAmountRDD = bothOver50SoldRDD.map {
       case (productId, (octAmount, novAmount)) =>
       (productId, octAmount + novAmount)
       }
over50SoldAndAmountRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[13] at map at <console>:26

scala> over50SoldAndAmountRDD.collect.foreach(println)
(8,140)
(15,131)
```



#브로드캐스트 변수로 실습

- 브로드캐스트 변수를 배포하기 위해 SparkContext에 대해 broadcast메소드를 호출하고 매개변수로 배포하고자 하는 데이터를 설정한다. 상품의 마스터 데이터가 로드된 productsMap을 설정하고 broadcast메소드를 호출하면 Broadcast[T] 클래스의 인스턴스를 결과값으로 돌려준다.

```scala
scala> import scala.collection.mutable.HashMap
import scala.collection.mutable.HashMap

scala> import java.io.{BufferedReader, InputStreamReader}
import java.io.{BufferedReader, InputStreamReader}

scala> import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.conf.Configuration

scala> import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.fs.{FileSystem, Path}

scala> val productsMap = new HashMap[String, (String, Int)]
productsMap: scala.collection.mutable.HashMap[String,(String, Int)] = Map()

scala> val hadoopConf = new Configuration
hadoopConf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml

scala> val fileSystem = FileSystem.get(hadoopConf)
fileSystem: org.apache.hadoop.fs.FileSystem = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-435449943_1, ugi=hadoop (auth:SIMPLE)]]

scala> val inputStream = fileSystem.open(new Path("/data/spark/products.csv"))
inputStream: org.apache.hadoop.fs.FSDataInputStream = org.apache.hadoop.hdfs.client.HdfsDataInputStream@15ddb82a

scala> val productsCSVReader = new BufferedReader(new InputStreamReader(inputStream))
productsCSVReader: java.io.BufferedReader = java.io.BufferedReader@2310e8a4

scala> var line = productsCSVReader.readLine
line: String = 1,가래떡(3개),16000

scala> while (line != null) {
       val splitLine = line.split(",")
       val productId = splitLine(0)
       val productName = splitLine(1)
       val unitPrice = splitLine(2).toInt
       productsMap(productId) = (productName, unitPrice)
       line = productsCSVReader.readLine
       }

scala> productsCSVReader.close()
```



- 마스터 데이터가 보관된 HashMap이 브로드캐스트 변수로서 각 이그제큐터에 배포된다.

```scala
scala> val broadcastedMap = sc.broadcast(productsMap)
broadcastedMap: org.apache.spark.broadcast.Broadcast[scala.collection.mutable.HashMap[String,(String, Int)]] = Broadcast(6)
```



- 두 달간 매달 50개 이상 판매된 상품을 찾아낸 뒤 (상품이름, 총 판매량, 총 매출) 형의 튜플을 만든다.

```scala
scala> val resultRDD = over50SoldAndAmountRDD.map {
       case (productId, amount) =>
       val productsMap = broadcastedMap.value
       val (productName, unitPrice) = productsMap(productId)
       (productName, amount, amount * unitPrice)
       }
resultRDD: org.apache.spark.rdd.RDD[(String, Int, Int)] = MapPartitionsRDD[14] at map at <console>:34

scala> resultRDD.collect.foreach(println)
(강정(10개),140,2100000)
(생과자(10개),131,2227000)
```



**#클러스터 환경에서의 공유 변수**

- 클러스터 환경에서 프레임워크들은 다수의 프로세스가 공유할 수 있는 읽기 자원과 쓰기 자원을 설정할 수 있도록 지원한다.

- 하둡은 분산캐시와 카운터를, 스파크는 브로드캐스트 변수와 어큐물레이터(Accumulators)를 제공하고 있다.

- 하둡의 분산 캐시는 단순히 대용량 파일을 전체 노드에서 쉽게 접근할 수 있게 하거나 단순히 숫자(카운트)를 증가시키는 것이 목적인 데 반해 스파크의 공유 변수는 단어 그대로 “읽거나 쓸 수 있는 공유 변수’의 의미로서 사용 목적에 따라 좀 더 범용적인 목적으로 활용할 수 있다.



**#브로드캐스트 변수**

- 스파크 잡이 실행되는 동안 클러스터 내의 모든 서버에서 공유 할 수 있는 읽기 전용 자원을 설정할 수 있는 변수

- 공유하고자 하는 데이터를 포함한 오브젝트를 생성하고 

- 오브젝트를 스파크 컨텍스트의 broadcast() 의 인자로 지정해 해당 메서드를 실행합니다

- 생성된 브로드캐스트 변수를 value() 를 통해 접근할 수 있습니다



**#accumulator**

- 쓰기 동작을 위한 것

- 클러스터 내의 모든 서버가 공유하는 쓰기 공간을 제공

- 각 서버에서 발생하는 특정 이벤트의 수를 세거나 관찰하고 싶은 정보를 모아 두는 등의 용도로 활용

- Org.apache.spark.util.AccumulatorV2 클래스를 상속받은 클래스를 정의하고 이 클래스의 인스턴스를 생성한다.

- accumulator 인스턴스를 스파크 컨텍스트가 제공하는 register()를 이용해 등록한다

- 스파크에서는 자주 사용되는 몇 가지 데이터 타입에 대한 어큐물레이터를 미리 정의해두었다.

- LongAccumulator, DoubleAccumulator, CollectionAccumulator

- 어큐뮬레이터를 증가시키는 동작은 클러스터의 모든 데이터 처리 프로세스에서 가능하지만  데이터를 읽는 동작은 드라이버 프로그램 내에서만 가능하다 

- RDD의 트랜스포메이션이나 액션 연산 내부에서는 어큐뮬레이터의 값을 증가시킬 수 만 있을 뿐 그 값을 참조해서 사용하는 것은 불가능하다

- 실습

```scala
scala> val questionnaireRDD =
       sc.textFile("/data/spark/questionnaire.csv").map { record =>
       val splitRecord = record.split(",")
       val ageRange = splitRecord(0).toInt / 10 * 10
       val maleOrFemale = splitRecord(1)
       val point = splitRecord(2).toInt
       (ageRange, maleOrFemale, point)
       }
questionnaireRDD: org.apache.spark.rdd.RDD[(Int, String, Int)] = MapPartitionsRDD[17] at map at <console>:30

scala> questionnaireRDD.cache
res5: questionnaireRDD.type = MapPartitionsRDD[17] at map at <console>:30

scala> val numQuestionnaire = questionnaireRDD.count
numQuestionnaire: Long = 19

scala> val totalPoints = questionnaireRDD.map(_._3).sum
totalPoints: Double = 64.0

scala> println(s"AVG ALL: ${totalPoints / numQuestionnaire}")
AVG ALL: 3.3684210526315788

scala> val (totalPoint, numQuestionnaire) =
       questionnaireRDD.map(record => (record._3, 1)).reduce {
       case ((intermedPoint, intermedCount), (point, one)) =>
       (intermedPoint + point, intermedCount + one)
       }
totalPoint: Int = 64
numQuestionnaire: Int = 19

scala> println(s"AVG ALL: ${totalPoints / numQuestionnaire}")
AVG ALL: 3.3684210526315788

scala> val agePointOneRDD = questionnaireRDD.map(record => (record._1, (record._3, 1))) 
agePointOneRDD: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[21] at map at <console>:30

scala> val totalPtAndCntPerAgeRDD = agePointOneRDD.reduceByKey {
       case ((intermedPoint, intermedCount), (point, one)) =>
       (intermedPoint + point, intermedCount + one)
       }
totalPtAndCntPerAgeRDD: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = ShuffledRDD[22] at reduceByKey at <console>:30

scala> totalPtAndCntPerAgeRDD.collect.foreach {
       case (ageRange, (totalPoint, count)) =>
       println(s"AVG Age Range($ageRange): ${totalPoint / count.toDouble}")
       }
AVG Age Range(30): 3.5
AVG Age Range(50): 1.5
AVG Age Range(40): 2.5
AVG Age Range(20): 3.7142857142857144
AVG Age Range(10): 4.0

scala> val numMAcc = sc.accumulator(0, "Number of M")
warning: there were two deprecation warnings; re-run with -deprecation for details
numMAcc: org.apache.spark.Accumulator[Int] = 0

scala> val totalPoint MAcc = sc.accumulator(0, "Total Points of M")
<console>:1: error: illegal start of simple pattern
val totalPoint MAcc = sc.accumulator(0, "Total Points of M")
                    ^

scala> val totalPointMAcc = sc.accumulator(0, "Total Points of M")
warning: there were two deprecation warnings; re-run with -deprecation for details
totalPointMAcc: org.apache.spark.Accumulator[Int] = 0

scala> val numFAcc = sc.accumulator(0, "Number of F")
warning: there were two deprecation warnings; re-run with -deprecation for details
numFAcc: org.apache.spark.Accumulator[Int] = 0

scala> val totalPointFAcc = sc.accumulator(0, "Total Points of F")
warning: there were two deprecation warnings; re-run with -deprecation for details
totalPointFAcc: org.apache.spark.Accumulator[Int] = 0

scala> questionnaireRDD.foreach {
       case (_, maleOrFemale, point) =>
       maleOrFemale match {
       case "M" =>
       numMAcc += 1
       totalPointMAcc += point
       case "F" =>
       numFAcc += 1
       totalPointFAcc += point
       }
       }

scala> println(s"AVG Male: ${totalPointMAcc.value / numMAcc.value.toDouble}")
AVG Male: 3.5

scala> println(s"AVG Female: ${totalPointFAcc.value / numFAcc.value.toDouble}")
AVG Female: 3.272727272727273
```



#DataFrame을 이용해 데이터처리 기술하기

- 샘플데이터

```
dessert-menu.csv================================================
D-0,초콜릿 파르페,4900,420
D-1,푸딩 파르페,5300,380
D-2,딸기 파르페,5200,320
D-3,판나코타,4200,283
D-4,치즈 무스,5800,288
D-5,아포가토,3000,248
D-6,티라미스,6000,251
D-7,녹차 파르페,4500,380
D-8,바닐라 젤라또,3600,131
D-9,카라멜 팬케익,3900,440
D-10,크림 안미츠,5000,250
D-11,고구마 파르페,6500,650
D-12,녹차 빙수,3800,320
D-13,초코 크레이프,3700,300
D-14,바나나 크레이프,3300,220
D-15,커스터드 푸딩,2000,120
D-16,초코 토르테,3300,220
D-17,치즈 수플레,2200,160
D-18,호박 타르트,3400,230
D-19,캬라멜 롤,3700,230
D-20,치즈 케익,4000,230
D-21,애플 파이,4400,350
D-22,몽블랑,4700,290


dessert-order.csv==================================
SID-0,D-0,2
SID-0,D-3,1
SID-1,D-10,4
SID-2,D-5,1
SID-2,D-8,1
SID-2,D-20,1
```



- 스파크 SQL 관련 SQLContext 작성

```scala
scala> val sqlContext = new HiveContext(sc)
warning: there was one deprecation warning; re-run with -deprecation for details
sqlContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@1ad6edb7

scala> import sqlContext.implicits._
import sqlContext.implicits._
```



- dessert-menu.csv의 한 줄을 한 레코드로 보고, 한 레코드 분의 데이터를 표현하는 Dessert 클래스를 케이스 클래스로서 정의

```scala
scala> case class Dessert(menuId: String, name: String, price: Int, kcal: Int)
defined class Dessert
```



- dessert-menu.csv로부터 RDD를 생성하고 map메소드를 호출해 Dessert클래스의 인스턴스를 요소로 하는 RDD로 변환. 이 RDD에 toDF메소드를 호출하면 RDD로부터 DataFrame이 생성된다.

```scala
scala> val dessertRDD = sc.textFile("/data/spark/dessert-menu.csv")
dessertRDD: org.apache.spark.rdd.RDD[String] = /data/spark/dessert-menu.csv MapPartitionsRDD[24] at textFile at <console>:36

scala> val dessertDF = dessertRDD.map { record =>
       val splitRecord = record.split(",")
       val menuId = splitRecord(0)
       val name = splitRecord(1)
       val price = splitRecord(2).toInt
       val kcal = splitRecord(3).toInt
       Dessert(menuId, name, price, kcal)
       }.toDF
dessertDF: org.apache.spark.sql.DataFrame = [menuId: string, name: string ... 2 more fields]
```



- 생성된 DataFrame의 스키마 정보 확인

```scala
scala> dessertDF.printSchema
root
 |-- menuId: string (nullable = true)
 |-- name: string (nullable = true)
 |-- price: integer (nullable = false)
 |-- kcal: integer (nullable = false)
```



- DataFrame으로부터 RDD 생성하기

```scala
scala> val rowRDD = dessertDF.rdd
rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[29] at rdd at <console>:37

scala> val nameAndPriceRDD = rowRDD.map { row =>
       val name = row.getString(1)
       val price = row.getInt(2)
       (name, price)
       }
nameAndPriceRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[30] at map at <console>:37

scala> nameAndPriceRDD.collect.foreach(println)
(초콜릿 파르페,4900)
(푸딩 파르페,5300)
(딸기 파르페,5200)
(판나코타,4200)
(치즈 무스,5800)
(아포가토,3000)
(티라미스,6000)
(녹차 파르페,4500)
(바닐라 젤라또,3600)
(카라멜 팬케익,3900)
(크림 안미츠,5000)
(고구마 파르페,6500)
(녹차 빙수,3800)
(초코 크레이프,3700)
(바나나 크레이프,3300)
(커스터드 푸딩,2000)
(초코 토르테,3300)
(치즈 수플레,2200)
(호박 타르트,3400)
(캬라멜 롤,3700)
(치즈 케익,4000)
(애플 파이,4400)
(몽블랑,4700)
```



- http://master:4040/jobs/ : 수행한 job을 확인할 수 있음



#DataFrame API로 DataFrame 다루기

- 컬럼과 식 선택

1. 

```scala
scala> val nameAndPriceDF = dessertDF.select(dessertDF("name"), dessertDF("price"))
nameAndPriceDF: org.apache.spark.sql.DataFrame = [name: string, price: int]

scala> nameAndPriceDF.printSchema
root
 |-- name: string (nullable = true)
 |-- price: integer (nullable = false)

scala> val selectAllDF = dessertDF.select("*")
selectAllDF: org.apache.spark.sql.DataFrame = [menuId: string, name: string ... 2 more fields]

scala> selectAllDF.printSchema
root
 |-- menuId: string (nullable = true)
 |-- name: string (nullable = true)
 |-- price: integer (nullable = false)
 |-- kcal: integer (nullable = false)

```



2. show 메소드로 데이터셋의 내용 확인

```scala
scala> nameAndPriceDF.show
+---------------+-----+
|           name|price|
+---------------+-----+
|  초콜릿 파르페| 4900|
|    푸딩 파르페| 5300|
|    딸기 파르페| 5200|
|       판나코타| 4200|
|      치즈 무스| 5800|
|       아포가토| 3000|
|       티라미스| 6000|
|    녹차 파르페| 4500|
|  바닐라 젤라또| 3600|
|  카라멜 팬케익| 3900|
|    크림 안미츠| 5000|
|  고구마 파르페| 6500|
|      녹차 빙수| 3800|
|  초코 크레이프| 3700|
|바나나 크레이프| 3300|
|  커스터드 푸딩| 2000|
|    초코 토르테| 3300|
|    치즈 수플레| 2200|
|    호박 타르트| 3400|
|      캬라멜 롤| 3700|
+---------------+-----+
only showing top 20 rows


scala> selectAllDF.show(3)
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|   D-0|초콜릿 파르페| 4900| 420|
|   D-1|  푸딩 파르페| 5300| 380|
|   D-2|  딸기 파르페| 5200| 320|
+------+-------------+-----+----+
only showing top 3 rows
```



3. 어느 DataFrame의 컬럼을 나타낼지 명확히 알 때에는 $"컬럼명"과 같이 표현할 수 있다.

```scala
scala> val nameAndDollarDF = nameAndPriceDF.select($"name", $"price" / lit(1000.0))
nameAndDollarDF: org.apache.spark.sql.DataFrame = [name: string, (price / 1000.0): double]

//lit함수를 이용해서 상수를 표현했지만 연산자의 오른쪽에 상수를 지정할 경우에는 lit 함수를 지정하지 않아도 된다.

scala> nameAndDollarDF.printSchema
root
 |-- name: string (nullable = true)
 |-- (price / 1000.0): double (nullable = true)
```



4. select 메소드로 선택한 (price / 1000.0)의 식에 dollar price라는 이름을 붙인다.

```scala
scala> val nameAndDollarDF = nameAndPriceDF.select (
     | $"name", ($"price" / lit(1000.0)) as "dollar price")
nameAndDollarDF: org.apache.spark.sql.DataFrame = [name: string, dollar price: double]

scala> nameAndDollarDF.printSchema
root
 |-- name: string (nullable = true)
 |-- dollar price: double (nullable = true)
```



- 필터링

1. dessert-menu.csv에서 5200원 이상의 메뉴만 추출하기

```scala
scala> val over5200WonDF = dessertDF.where($"price" >= 5200)
over5200WonDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [menuId: string, name: string ... 2 more fields]

scala> over5200WonDF.show
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|   D-1|  푸딩 파르페| 5300| 380|
|   D-2|  딸기 파르페| 5200| 320|
|   D-4|    치즈 무스| 5800| 288|
|   D-6|     티라미스| 6000| 251|
|  D-11|고구마 파르페| 6500| 650|
+------+-------------+-----+----+
```



2. where메소드와 select메소드를 활용해 메뉴의 name컬럼만 선택하기

```scala
scala> val over5200WonNameDF = dessertDF.where($"price" >= 5200).select($"name")
over5200WonNameDF: org.apache.spark.sql.DataFrame = [name: string]

scala> over5200WonNameDF.show
+-------------+
|         name|
+-------------+
|  푸딩 파르페|
|  딸기 파르페|
|    치즈 무스|
|     티라미스|
|고구마 파르페|
+-------------+
```



- 정렬

1. 메뉴를 가격의 오름차순, 칼로리의 내림차순으로 정렬하기

```scala
scala> val sortedDessertDF = dessertDF.orderBy($"price".asc, $"kcal".desc)
sortedDessertDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [menuId: string, name: string ... 2 more fields]

scala> sortedDessertDF.show
+------+---------------+-----+----+
|menuId|           name|price|kcal|
+------+---------------+-----+----+
|  D-15|  커스터드 푸딩| 2000| 120|
|  D-17|    치즈 수플레| 2200| 160|
|   D-5|       아포가토| 3000| 248|
|  D-14|바나나 크레이프| 3300| 220|
|  D-16|    초코 토르테| 3300| 220|
|  D-18|    호박 타르트| 3400| 230|
|   D-8|  바닐라 젤라또| 3600| 131|
|  D-13|  초코 크레이프| 3700| 300|
|  D-19|      캬라멜 롤| 3700| 230|
|  D-12|      녹차 빙수| 3800| 320|
|   D-9|  카라멜 팬케익| 3900| 440|
|  D-20|      치즈 케익| 4000| 230|
|   D-3|       판나코타| 4200| 283|
|  D-21|      애플 파이| 4400| 350|
|   D-7|    녹차 파르페| 4500| 380|
|  D-22|         몽블랑| 4700| 290|
|   D-0|  초콜릿 파르페| 4900| 420|
|  D-10|    크림 안미츠| 5000| 250|
|   D-2|    딸기 파르페| 5200| 320|
|   D-1|    푸딩 파르페| 5300| 380|
+------+---------------+-----+----+
only showing top 20 rows
```



- 집약처리

1. 각 메뉴의 칼로리의 평균을 avg함수를 이용해 계산해보기

```scala
scala> val avgKcalDF = dessertDF.agg(avg($"kcal") as "avg_of_kcal")
avgKcalDF: org.apache.spark.sql.DataFrame = [avg_of_kcal: double]

scala> avgKcalDF.show
+-----------------+
|      avg_of_kcal|
+-----------------+
|291.7826086956522|
+-----------------+
```



2. 그룹단위로 집약처리 하기. groupBy메소드의 매개변수에 가격대를 지정해 같은 가격대의 메뉴를 그룹화한다. 그리고 그룹 단위로 분류된 결과에 agg 메소드를 호출하고 집약함수 count를 이용해 가격대 단위로 메뉴가 몇 개인지 계산할 수 있다.

```scala
scala> val numPerPriceRangeDF = dessertDF.groupBy (
    
    //price 칼럼을 1000으로 나누고, IntegerType으로 변환해 1000을 곱하는 식으로 가격대를 계산한다.
       (($"price" / 1000) cast IntegerType) * 1000
       as "price_range").agg(count($"price")).orderBy($"price_range")
numPerPriceRangeDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [price_range: int, count(price): bigint]

scala> numPerPriceRangeDF.show
+-----------+------------+                                                      
|price_range|count(price)|
+-----------+------------+
|       2000|           2|
|       3000|           9|
|       4000|           6|
|       5000|           4|
|       6000|           2|
+-----------+------------+
```





- DataFrame끼리의 결합

1. dessert-menu.csv와 dessert-order.csv의 데이터셋을 메뉴ID로 결합

```scala
scala> case class DessertOrder(sId: String, menuId: String, num: Int)
defined class DessertOrder

scala> val dessertOrderRDD = sc.textFile("/data/spark/dessert-order.csv")
dessertOrderRDD: org.apache.spark.rdd.RDD[String] = /data/spark/dessert-order.csv MapPartitionsRDD[78] at textFile at <console>:39

scala> val dessertOrderDF = dessertOrderRDD.map { record =>
     | val splitRecord = record.split(",")
     | val sId = splitRecord(0)
     | val menuId = splitRecord(1)
     | val num = splitRecord(2).toInt
     | DessertOrder(sId, menuId, num)
     | }.toDF
dessertOrderDF: org.apache.spark.sql.DataFrame = [sId: string, menuId: string ... 1 more field]
```



2. 전표별, 메뉴별 매출 계산

```scala
scala> val amntPerMenuPerSlipDF = dessertDF.join(
     | dessertOrderDF, dessertDF("menuId") === dessertOrderDF("menuId"), "inner"
    //결합 방법을 생략하면 inner를 지정했을 때와 동일하게 움직인다.
     | ).select($"sId", $"name", ($"num" * $"price")
     | as "amount_per_menu_per_slip" )
amntPerMenuPerSlipDF: org.apache.spark.sql.DataFrame = [sId: string, name: string ... 1 more field]

scala> amntPerMenuPerSlipDF.show
+-----+-------------+------------------------+                                  
|  sId|         name|amount_per_menu_per_slip|
+-----+-------------+------------------------+
|SID-0|     판나코타|                    4200|
|SID-2|     아포가토|                    3000|
|SID-1|  크림 안미츠|                   20000|
|SID-2|    치즈 케익|                    4000|
|SID-2|바닐라 젤라또|                    3600|
|SID-0|초콜릿 파르페|                    9800|
+-----+-------------+------------------------+
```



3. 전표별 총 매출 계산 => 전표 ID로 그룹화하고 집약함수 sum을 이용한다.

```scala
scala> val amntPerSlipDF = amntPerMenuPerSlipDF.groupBy($"sId").agg(
     | sum($"amount_per_menu_per_slip") as "amount_per_slip"
     | ).select($"sId", $"amount_per_slip")
amntPerSlipDF: org.apache.spark.sql.DataFrame = [sId: string, amount_per_slip: bigint]

scala> amntPerSlipDF.show
+-----+---------------+                                                         
|  sId|amount_per_slip|
+-----+---------------+
|SID-0|          14000|
|SID-1|          20000|
|SID-2|          10600|
+-----+---------------+
```





#UDF 이용하기



- 스파크 SQL의 UDF 이용하기

```scala
scala> val strlen = sqlContext.udf.register("strlen", (str: String) => str.length)
strlen: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,IntegerType,Some(List(StringType)))

scala> sqlContext.sql("SELECT strlen('Hello Spark SQL') AS result_of_strlen").show
+----------------+
|result_of_strlen|
+----------------+
|              15|
+----------------+
```



#구조화된 각종 데이터셋 다루기



- 파일 형식의 구조화된 데이터셋 다루기

```scala
scala> val dfWriter = dessertDF.write
dfWriter: org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row] = org.apache.spark.sql.DataFrameWriter@6a57a878

scala> dfWriter.format("parquet").save("/data/spark/dessert_parquet")

scala> val dessertDF2 = dfReader.format("parquet").load("/data/spark/dessert_parquet")
dessertDF2: org.apache.spark.sql.DataFrame = [menuId: string, name: string ... 2 more fields]

scala> dessertDF2.orderBy($"name").show(3)
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|  D-11|고구마 파르페| 6500| 650|
|  D-12|    녹차 빙수| 3800| 320|
|   D-7|  녹차 파르페| 4500| 380|
+------+-------------+-----+----+
only showing top 3 rows
```



- 테이블 형식의 구조화된 데이터셋 다루기

```scala
scala> dessertDF.write.format("parquet").saveAsTable("dessert_tbl_parquet")

scala> sqlContext.read.format("parquet").table("dessert_tbl_parquet").show(3)
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|   D-0|초콜릿 파르페| 4900| 420|
|   D-1|  푸딩 파르페| 5300| 380|
|   D-2|  딸기 파르페| 5200| 320|
+------+-------------+-----+----+
only showing top 3 rows


scala> sqlContext.sql("SELECT * FROM dessert_tbl_parquet LIMIT 3").show
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|   D-0|초콜릿 파르페| 4900| 420|
|   D-1|  푸딩 파르페| 5300| 380|
|   D-2|  딸기 파르페| 5200| 320|
+------+-------------+-----+----+
```

