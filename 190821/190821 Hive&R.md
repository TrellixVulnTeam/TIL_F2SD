# 190821 Hive & R 연동하기

(어제꺼에 이어서)

#metastore 로 사용할 database 생성 및 metastore에 스키마 생성

```
[hadoop@master ~]$ su -
[root@master ~] mysql -u root -p
Enter password:
mysql> show databases;
mysql> CREATE DATABASE metastore_db;

mysql> USE metastore_db;
mysql> show tables;
mysql> SOURCE /usr/local/hive/scripts/metastore/upgrade/mysql/hive-schema-1.1.0.mysql.sql;
mysql> show tables;
```



#$HIVE_HOME/lib 아래 mysql-connector-java-5.1.36-bin.jar에 복사 

```
[hadoop@master ~]$ tar -xvf ./Downloads/mysql-connector-java-5.1.36.tar.gz
[hadoop@master ~]$ ls
[hadoop@master ~]$ cd  /home/hadoop/mysql-connector-java-5.1.36/
[hadoop@master ~]$ cp  mysql-connector-java-5.1.36-bin.jar /usr/local/hive/lib/
```



#하둡 시작

```
[hadoop@master ~]$ cd /usr/local/hadoop-2.7.7/sbin
[hadoop@master ~]$ ./start-all.sh
[hadoop@master ~]$ hive
```



#잘 연동됐는지 확인하기 위해 test 테이블 생성

```
hive> show databases;
hive> create database test_db;
hive> use test_db
hive> create table test ( name  varchar(10) );
hive> describe test
```



#하둡 DFS에 데이터베이스와 테이블은 디렉토리로 생성됨을 확인

```
[hadoop@master ~]$ hadoop fs -ls -R /user/
```



#metastore에서 생성한 데이터베이스와 테이블 메타 정보 확인

```
mysql> select OWNER, TBL_NAME, TBL_TYPE from TBLS;
mysql> select OWNER_NAME, OWNER_TYPE, NAME from DBS;
```



여기까지가 연동 확인!

-------



#hive  -f  <파일명> : HiveQL이 기술되어 있는 파일을 인수로 사용하여 명령어 라인에서 실행     

#hive  -e  ‘HiveSQL문’ : 명령어 라인 인수로 HiveQL문을 직접 기술해서 실행

#hive  -s  -e  ‘HiveSQL문’  : Hive는 실행 시에 Hive에 관한 로그나 MapREduce 잡 실행 상태에 대한 정보를 출력하지만, -s 인수를 지정하면 silent 모드가 돼서 출력을 억제하는 것이 가능

- Hive 명령을 실행함으로써 Hive 쉘을 시작할 수 있다

- SET 명령을 통해 Hadoop이나 Hive 관련 속성을 설정 할 수 있다.  (설정한 것은 Hive 쉘 내에서만 유효하다) 

  ```
  hive> SET mapred.reduce.tasks= 20
  Hive> SET  hive.parallel.queries=true;
  ```

  



- LIST 명령으로 이용 가능한 리소스를 확인할 수 있다

- !마크를 붙임으로써 OS 명령을 실행할 수 있다

- fs 명령을 사용해서 HDFS 조작 관련 명령을 실행 할 수 있다.

- exit Hive  종료할 수 있다

- Hive에서는 데이터베이스와 스키마가 동일한 것을 가리킨다.

  ```sql
  CREATE  DATABASE  db1  ;     or    CREATE  SCHEMA   db1 ;
  USE  db1 ;
  SHOW  TABLES;
  ```

  

- Hive 테이블에 데이터를 등록할 때는 입력 데이터가 파일 시스템 상에 있으면 LOAD,  Hive 테이블이면 INSERT를 이용한다.

- Hive 테이블 데이터를 파일 시스템 상의 파일로 출력할 때도 INSERT문을 이용한다.

  ```sql
  LOAD  DATA  [LOCAL]  INPATH   ‘<파일 경로>’  [OVERWRITE]  INTO  TABLE   <테이블명>  [PARTITION  (partcol1 = val1, partcol2=col2,…)]
  ```

  ```sql
  INSERT  OVERWRITE  TABLE  <테이블명>  [PARTITION  (partcol1 = val1, partcol2=col2,…)]    [IF  NOT  EXISTS]]  <SELECT구>  FROM  <FROM 구> ;
  ```

  

- HiveQL은  JOIN,  서브쿼리, UNION ALL을 지원한다.

- GROUP BY는  Reduce에서 처리하지만, Map에서 처리하도록 명시적으로 지정할 수 있다.  Map 태스크의  메모리 사용량이 늘어날 가능성이 있지만, Map에서 집계할 수 있으므로 Reduce로 보내는 전송량을 줄 일 수 있다. 

- Map에서 GROUP BY를 실행하려면  hive.map.agg 속성을 true로 한다

- ORDER BY로 출력 결과 전체를 정렬할  때는 reducer 수가 하나로 제한한다



- SORT BY를 지정하면  Reducer를 다수 동작시킬 수 있으며, 처리 결과를 Reducer내에 정렬한다.

- SORT By는 Hadoop의 Shuffle 정렬 기능을 이용한다.
  Shuffle은 Reduce 처리 단위로 실행되기 때문에 Select문과 같이 모든 데이터를 정렬한 결과를 얻을 수 없다.

  ```sql
  SELECT  col1  FROM  table1  SORT BY col1  ASC;
  ```

  

- 특정 행을 어떤 Reducer로 보낼지 제어하고 싶을 때는 CLUSTER BY 또는 DISTRIBUTE BY를 이용한다.

- SELECT문의 출력 건수를 LIMIT 구로 제한할 수 있다 

  ```sql
  SET  mapred.reduce.tasks = 1
  SELECT  *  FROM  table1  SORT BY  col1  DESC  LIMIT 5
  ```

  

- 정렬한 결과를 상위 다섯 건만 출력하려면, SET문으로 reducer 수를 1로 제한한 후에 SORT BY로 정렬하고, 출력 건수를 LIMIT로 제한한다.

- SELECT 부분에 자바의 정규 표현식을 기술 할 수 있다 

  ```sql
  SELECT  ‘(ds|hr)?+.+ FROM  sales
  ```



- Col1의 값이 같은 것은 동일 reducer로 처리 할 수 있다.

  ```sql
  SELECT  col1,  col2  FROM  table1  DISTRIBUTE BY col1;
  ```

  

- DISTRIBUTE  BY에  SORT BY를 부여하면 Reducer 출력을 정렬할 수 있다 

  ```sql
  SELECT  col1,  col2  FROM  table1  DISTRIBUTE BY col1  SORT BY  col1  ASC,  col2 DESC;
  ```



- DISTRIBUTE  BY와  SORT BY가 같은 컬럼을 지정해야 한다면,  CLUSTER BY를 사용해 동일한 기능을  간단히 구현할 수 있다. 

  ```sql
  SELECT  col1,  col2   FROM  table1  CLUSTER  BY col1  ;
  ```



- Hive에서는 내부 결합, 외부 결합, 반 결합, 크로스 결합을 이용할 수 있다.

- HiveQL의 JOIN은 ON 구 안에 NOT 조건을 기술할 수 없다.

- WHERE 구내의 IN 및 EXISTS 서브 쿼리는 Hive에서 지원하지 않는다. 동일 처리를 구현하려면 SEMI  LEFT  JOIN을 사용하면 된다.  



- SEMI  LEFT  JOIN을 사용할 때, 오른쪽 테이블은  SELECT 구에서 사용할 수 없고 ON 구 내에서만 표현할 수 있다

  ```sql
  SELECT  table1.col1,  table1.col2  FROM  table1  LEFT  SEMI  JOIN  table2  ON  (table1.col1 = table2.col1);
  ```

  

- Hive QL에서는 UNION ALL을 이용할 수 있다. 

- UNION ALL 구를 사용하는 경우 SELECT문에서 지정한 컬럼은 동일 데이터형이어야 한다.

- HiveQL에서는 상위 계층의 UNION ALL은 지원하지 않으며, 서브 쿼리 내부에서 이용합니다. 

  ```sql
  SELECT  col1,  col2  FROM   (
          SELECT  col1,  col2  FROM  input1   UNION  ALL col1 
          SELECT  col1,  col2  FROM  input2
  )  tmp ;
  ```





**#External Table 실습**

- External Table : 테이블의 데이터는 데이터베이스 테이블에 저장돼 있고 테이블의 구조정보만 db에 저장돼있는 테이블

1) airline_db 데이터베이스 생성

```
hive> create database airline_db ;
hive> use airline_db ;
```

2) 테이블 생성

```
hive> CREATE EXTERNAL TABLE airline (
    > Year string,
    > Month string,
    > DayofMonth string,
    > DayOfWeek string,
    > DepTime string,
    > CRSDepTime string,
    > ArrTime string,
    > CRSArrTime string,
    > UniqueCarrier string,
    > FlightNum string,
    > TailNum string,
    > ActualElapsedTime string,
    > CRSElapsedTime string,
    > AirTime string,
    > ArrDelay string,
    > DepDelay string,
    > Origin string,
    > Dest string,
    > Distance string,
    > TaxiIn string,
    > TaxiOut string,
    > Cancelled string,
    > CancellationCode string,
    > Diverted string,
    > CarrierDelay string,
    > WeatherDelay string,
    > NASDelay string,
    > SecurityDelay string,
    > LateAircraftDelay  string
    > )
    > ROW FORMAT DELIMITED
    >  FIELDS TERMINATED BY ',' 
    >  LINES TERMINATED BY '\n'
    > LOCATION '/data/airline/';
```



3) 월별 도착지연횟수를 출력하는 select문

```
hive> SELECT Year,Month, count(DepDelay)
      FROM airline
      GROUP BY Year,Month
      SORT BY Year,Month;   --reducer 별 처리 데이터 정렬, 전체 결과 정렬되지 않음

hive> SELECT Year,Month, count(DepDelay)
      FROM airline
      GROUP BY Year,Month
      ORDER BY Year,Month;   ----reducer개수 1개로 제한, 전체 정렬

hive> explain SELECT Year,Month, count(DepDelay)
      FROM airline
      GROUP BY Year,Month
      SORT BY Year,Month;
```





**#text파일에 저장한 정보를 db에 넣기**

 1) [hadoop@master ~]$ vi /home/hadoop/dept.txt  다음과 같이 설정한다. 

```
10,'ACCOUNTING','NEW YORK'
20,'RESEARCH','DALLAS'
30,'SALES','CHICAGO'
40,'OPERATIONS','BOSTON'
```



2) dept 테이블 생성하고 데이터 넣기

```
hive> CREATE TABLE IF NOT EXISTS dept (
deptno INT, dname STRING, loc STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

hive> describe dept

hive> load data local inpath '/home/hadoop/dept.txt' 
      overwrite into table dept;
hive> select  * from dept;
OK
10	'COUNTING'	'NEW YORK'
20	'RESEARCH'	'DALLAS'
30	'SALES'	'CHICAGO'
40	'OPERATIONS'	'BOSTON'

=> dept.txt에 저장된 메타정보가 dept 테이블에 들어간다.

hive> !hadoop fs -ls /user/hive/warehouse/
```





**#조인으로 carriers 테이블과 airline 테이블 합쳐서 데이터 fetch 하기**

1. carriers.csv파일을 carriers테이블을 생성하고, 데이터 로딩

```
hive> CREATE TABLE IF NOT EXISTS carriers (
       UniqueCarrier string,
       CarrierFullName String
       ) ROW FORMAT DELIMITED
         FIELDS TERMINATED BY ',' 
         LINES TERMINATED BY '\n'
        LOCATION '/data/metadata/';
   
hive> describe carriers;

hive> select * from carriers limit 5;
OK
Code	Description
"02Q"	"Titan Airways"
"04Q"	"Tradewind Aviation"
"05Q"	"Comlux Aviation
"06Q"	"Master Top Linhas Aereas Ltd."

hive> !hadoop fs -ls /user/hive/warehouse/ ;
```



2. airlineinfo 테이블 생성

```
   hive> CREATE TABLE IF NOT EXISTS airlineinfo (
   UniqueCarrier string,
   CarrierFullName String,
   FlightNum string,
   TailNum string,
   Dest string,
   Distance string,
   Cancelled string
   );

hive> describe airlineinfo
hive> !hadoop fs -ls /user/hive/warehouse/
Found 2 items
drwxr-xr-x   - hadoop supergroup          0 2019-08-21 22:12 /user/hive/warehouse/airline_db.db
drwxr-xr-x   - hadoop supergroup          0 2019-08-21 20:17 /user/hive/warehouse/airlineinfo
```



3. airline테이블과 carriers테이블의 조인 결과를 airlineinfo 테이블에 로딩

```
hive> INSERT  OVERWRITE  TABLE  airlineinfo 
 select  a.UniqueCarrier ,
   b.CarrierFullName ,
   a.FlightNum,
   a.TailNum ,
   a.Dest ,
   a.Distance ,
   a.Cancelled 
 from  airline a , carriers b  
where a.UniqueCarrier = substr(b.UniqueCarrier, 2, 2);

hive> select * from airlineinfo limit 10;
OK
WN	"Southwest Airlines Co."	2891	N351	ONT	389	0
WN	"Wings Airways"	2891	N351	ONT	389	0
WN	"Winair Inc."	2891	N351	ONT	389	0
WN	"Southwest Airlines Co."	462	N370	PDX	479	0
WN	"Wings Airways"	462	N370	PDX	479	0
WN	"Winair Inc."	462	N370	PDX	479	0
WN	"Southwest Airlines Co."	1229	N685	PDX	479	0
WN	"Wings Airways"	1229	N685	PDX	479	0
WN	"Winair Inc."	1229	N685	PDX	479	0
WN	"Southwest Airlines Co."	1355	N364	PDX	479	0

hive> !hadoop fs -ls /user/hive/warehouse/
```



-----------------

## Hadoop R 연동

- R은 데이터 분석을 위한 통계 및 그래픽스를 지원하는 자유 소프트웨어 환경이다

- R은 벨 연구소에서 만들어진 통계 분석 언어인 S에 두고 있다

- R은 현재 데이터 분석을 위한 도구로 많은 인기를 누리고 있다. 

- R은 컴퓨터 언어이자 다양한 패키지의 집합이다

- R 안에서 대부분의 데이터 분석을 해낼 수 있다는 장점이 있다.

- R은 통계, 기계 학습, 금융, 생물정보학, 그래픽스에 이르는 다양한 통계 패키지를 갖추고 있으며 이 모든 것이 무료로 제공된다.

- R의 다양한 패키지는 CRAN(http://cran.r-project.org/web/views/)을 통해 한곳에서 살펴볼 수 있다. 

- R은 공개 소프트웨어로 http://www.r-project.org/에서 다운로드해서 설치할 수 있다



#R연동하기

1. 필요 파일 설치 및 권한 설정

```
[root@master ~]# yum install epel-release
[root@master ~]# yum install npm
[root@master ~]# yum install R  // 설치

[root@master ~]# ls -l /usr/lib64
[root@master ~]# chown -R hadoop:hadoop /usr/lib64/R
[root@master ~]# ls -l /usr/lib64  //설치 확인 및 권한설정
```



2. hadoop의 .bash_profile에 추가

```
[hadoop@master ~]$ vi .bash_profile => 아래와 같이 수정
export HADOOP_CMD=/usr/local/hadoop-2.7.7/bin/hadoop
export HADOOP_STREAMING=/usr/local/hadoop-2.7.7/share/hadoop/tools/lib/hadoop-streaming-2.7.7.jar

[hadoop@master ~]$ source ./.bash_profile  //해당 터미널에서 .bash_profile 동기화
[hadoop@master ~]$ R  //R 실행
```



3. 설치

```
> install.packages(c("rJava", "Rcpp", "RJSONIO", "bitops", "digest", "functional", "stringr", "plyr", "reshape2", "caTools"))

https://github.com/RevolutionAnalytics/RHadoop/wiki => 필요 tar 파일 다운로드 후

> install.packages("/home/hadoop/Downloads/rhdfs_1.0.8.tar.gz", repos=NULL, type="source")

> install.packages("/home/hadoop/Downloads/rmr2_3.3.1.tar.gz", repos=NULL, type="source")
> install.packages("/home/hadoop/Downloads/plyrmr_0.6.0.tar.gz", repos=NULL, type="source")
> install.packages("/home/hadoop/Downloads/rhbase_1.2.1.tar.gz", repos=NULL, type="source")
> install.packages("/home/hadoop/Downloads/ravro_1.0.4.tar.gz", repos=NULL, type="source")
> install.packages(c("bit64", "rjson"))  // 필요 파일들 인스톨
```



#R 실습하기

1. vi test.R 생성하고 다음과 같이 내용 적어 저장

```
[hadoop@master ~]$ vi test.R  => 안의 내용을 다음과 같이 적는다. 
print("R running~ from source")
a<-  seq(1,100,by=2)  //R에서는 =대신 <-를 많이 사용한다! 1~100까지 2씩 차이나게!
print(class(a))
print(a)
```



2. R실행해서 다음과 같이 명령어 실행

```
> source("/home/hadoop/test.R") //실행
[1] "R running~ from source"
[1] "numeric"
 [1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49
[26] 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99  //결과
```





#R 명령어

- 작업 디렉토리 설정 

```
>setwd(“디렉터리명”)
```



- print() 함수는 1번에 1가지만 출력, cat() 함수는 숫자와 문자 여러 개를 한번에 출력

- 여러 개의 명령을 연속적으로 실행하고 싶을 경우에 세미콜론(;)을 사용

```
> print(3, 4)
> cat(1, ‘a’, 2, ‘b’)
> 1+2 ; 3*2 ; 4/2
```



- 산술연산자

  - /         나누기 (실수 가능)

  - %/%    정수 나누기

  - %%     나머지 구하기

  - ^, **    승수 구하기



- 데이터 타입 확인할때는 class 함수 사용  

```
> class(‘1’)
> class(1)
```



- NA – Not Applicable. 값이 있어도 정해진 범위 안에 있는 값이 아니라서 사용할 수 없는 경우.

  -  is.na(변수명)는 NA값인지 아닌지 확인

  ```
  sum(1, NA, 2)  # NA를 더하므로 결과가 NA로 출력됩니다.  => NA
  sum(1, NULL, 2)   #NULL값을 제외하고 나머지 값만 더해서 결과 출력  => 3
  sum(1, 2, NA, na.rm=T)  # NA값을 제거하고 올바른 계산을 수행  => 3
  ```



- Factor 형
  - 여러 번 중복으로 나오는 데이터들을 각 값으로 모아서 대표 값을 출력해 주는 형태
  - stringsAsFactors=FALSE 옵션은 대표값으로 정리하지 않고 중복되는 상태 그대로 사용하게 해준다.

  ```
  > c(‘a’, ‘a’, ‘b’, ‘b’, ‘c’, ‘b’)
  > factor(c(‘a’, ‘a’, ‘b’, ‘b’, ‘c’, ‘b’))
  ```

  



#실습1. 1부터 1,000까지의 숫자를 생성/ 각 숫자 모두를 제곱하는 연산을 수행

- [hadoop@master ~]$ vi test2.R

```
library(rhdfs) # Rhadoop package for hdfs
>hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
library(rmr2)  # RHadoop package for MapReduce

hadoop fs -mkdir /tmp/ex1

 dfs.rmr("/tmp/ex1")
 small.ints <- to.dfs(1:1000, "/tmp/ex1")

 result <- mapreduce(input = small.ints, 
	map = function(k,v) cbind(v,v^2)
)
 out <- from.dfs(result)
 print(out)
```



- [hadoop@master ~]$ R

```
> source("/home/hadoop/test2.R")
```



#실습2. WordCount

- [hadoop@master ~]$ vi wordtest.R

```
library(rhdfs) # Rhadoop package for hdfs
hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
library(rmr2)  # RHadoop package for MapReduce
 
inputfile <- "/tmp/README.txt"
if(!hdfs.exists(inputfile)) stop("File is not found")
outputfile <- "/tmp/ex4"
if(hdfs.exists(outputfile)) hdfs.rm(outputfile)
 
map <- function(key, val){
	words.vec <- unlist(strsplit(val, split = " "))
	#lapply(words.vec, function(word) 
    keyval(words.vec, 1)
}
 
reduce <- function(word, counts ) {
	keyval(word, sum(counts))
}
 
result <- mapreduce(input = inputfile,
	output = outputfile, 
	input.format = "text", 
	map = map, 
	reduce = reduce, 
	combine = T
)
 
## wordcount output
freq.dfs <- from.dfs(result)
freq <- freq.dfs$val
word <- freq.dfs$key
oidx <- order(freq, decreasing=T)[1:10]
 
# Words frequency plot
barplot(freq[oidx], names.arg=word[oidx] )
```



- [hadoop@master ~]$ R

```
> source("/home/hadoop/wordtest.R")
```



※단 이거 실행하기 전에 메모리 부족하면 늘려주고 하자!