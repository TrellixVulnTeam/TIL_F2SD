# 190819 Hadoop

**#라이브 노드가 제대로 생성되지 않을 때** 

- master, slave1에서 root계정으로
  - ifconfig로 ip 확인
  - vi /etc/hosts 수정 
  - vi /etc/sysconfig/iptables 수정
  - 방화벽 설정
    - systemctl stop iptables
    - systemctl start iptables (껐다 켜기)

- hadoop계정으로 hadoop 시작
  - cd /usr/local/hadoop-2.7.7/sbin
  - ./start-all.sh
  - http://master:50070/dfshealth.html  에서 라이브노드 개수 확인



#항공편 운항 통계데이터 처리하기

- 데이터 관련 정보
  - 다운로드 경로 : http://stat-computing.org/dataexpo/2009/the-data.html
  - 미국 항공편 운항 통계 데이터, 1987년부터 2008년까지 미국 내 모든 상업 항공편에 대한 항공편 도착과 출발 세부 사항에 대한 정보를 제공
  - csv 파일, 총 29개 컬럼으로 구성, 컬럼 정보는 콤마(,)를 기준으로 구분
  - 총 파일 크기 11GB(총 압축 크기 1.6GB)
  - 분석용 데이터 업로드 : hadoop fs -put   운항 통계 데이터파일  /airline 



- 받은 데이터의 csv파일 압축을 풀고 home/hadoop/data와 output에 넣기
  - bunzip2 ./Downloads/2008.csv.bz2 => 압축풀기
  - hadoop fs -mkdir /output/airline
    hadoop fs -mkdir /data/airline       => airline디렉토리 만들기



- 코드 만들어서 jar파일 생성하기

  - Mapper 클래스

  ```java
  package lab.hadoop.airline;
  import java.io.IOException;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Mapper;
  
  public class DepartureDelayCountMapper extends
  		Mapper<LongWritable, Text, Text, IntWritable> {
  	
  	//map print value
  	private final static IntWritable outputValue = new IntWritable(1);
  	
  	//map print key
  	private Text outputKey = new Text();
  	
  		public void map(LongWritable key, Text value, Context context)  
  				throws IOException, InterruptedException {
  		    if (key.get() > 0) {
  			// 콤마 구분자 분리
  			String[] colums = value.toString().split(",");
  			if (colums != null && colums.length > 0) {
  			    try {
  				// 출력키 설정
  				outputKey.set(colums[0] + "," + colums[1]);
  				if (!colums[15].equals("NA")) {
  					int depDelayTime = Integer.parseInt(colums[15]);
  					if (depDelayTime > 0) {
  						// 출력 데이터 생성
  						context.write(outputKey, outputValue);
  						}
  					}
  			         } catch (Exception e) {
  				e.printStackTrace();
  				}
  			}
  	     }
  	}
  }
  ```

  

  - Reducer 클래스

  ```java
  package lab.hadoop.airline;
  import java.io.IOException;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Reducer;
  
  public class DelayCountReducer extends
  		Reducer<Text, IntWritable, Text, IntWritable>{
  	
  	private IntWritable result = new IntWritable();
  	
  	//Total Departure Delay Time by Month
  	public void reduce(Text key, Iterable<IntWritable> values, Context context) 
              throws IOException, InterruptedException {
  		int sum = 0;
  		for (IntWritable value : values)
  		sum += value.get();
  		result.set(sum);
  		context.write(key, result);
  		}
  }
  ```

  

  - Driver 클래스

  ```java
  //driver 작업
  package lab.hadoop.airline;
  import java.io.IOException;
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.fs.FileSystem;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
  
  
  public class DepartureDelayCount {
     public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Configuration conf= new Configuration();
        if(args.length !=2) {
           System.err.println("Usage : DepartureDelayCount <input> <output>");
           System.exit(2);
        }
        // create file system control reference
        FileSystem fs=FileSystem.get(conf);
        // set path
        Path path=new Path(args[1]);
        if(fs.exists(path)) {
           fs.delete(path, true);
        }
        // set job name
        @SuppressWarnings("deprecation")
  	Job job= new Job(conf, "DepartureDelayCount");
        
        //set in-output data path
        FileInputFormat.addInputPath(job, new Path(args[0])); //arrange input path
        FileOutputFormat.setOutputPath(job, new Path(args[1])); //arrange output path
     
        job.setJarByClass(DepartureDelayCount.class); //set main class 
        job.setMapperClass(DepartureDelayCountMapper.class); //set mapper class
        job.setReducerClass(DelayCountReducer.class); //set reducer class
        
        job.setInputFormatClass(TextInputFormat.class); // set input format 
        job.setOutputFormatClass(TextOutputFormat.class); //set output format
        //output key, value type 
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
     
        job.waitForCompletion(true);
        
     }
  }
  ```



- Export한 jar파일을 옮기고 데이터 출력
  - hadoop jar ./departure.jar /data/airline /output/airline  => Export한 jar 파일을 output
  - hadoop fs -cat /output/airline/part-r-00000 => output한 결과값 호출



-----------------

- org.apache.hadoop.util.GenericOptionsParser

  - 하둡 콘솔 명령에서 입력한 옵션을 분석.

  - 네임노드, 잡트래커, 추가 구성 자원 등을 설정할 수 있는 옵션 제공
  - getRemainingArgs() : String[]
  - GenericOptionsParser에서 제공하는 파라미터를 제외한 나머지 파라미터를 반환  

- org.apache.hadoop.util.Tool
  - GenericOptionsParser의 콘솔 설정 옵션을 지원하기 위한 인터페이스
  - interface Tool extends Configurable

  - 드라이버 클래스 정의 시 Configured를 상속받고, Tool 인터페이스를 구현
  - public class DelayCount extends Configured implements Tool
  -  Configured 클래스는 환경설정 정보를 제어할 수 있게 한다.
  - Tool 인터페이스는 사용자의 옵션을 조회할 수 있게 한다.  

- org.apache.hadoop.util.ToolRunner
  - Tool 인터페이스의 실행을 도와주는 헬퍼 클래스
  - GenericOptionsParser를 사용해 사용자가 콘솔 명령어에서 설정한 옵션을 분석하고, Configuration 객체에 설정.

  - Configuration 객체를 Tool 인터페이스에 전달한 후, Tool 인터페이스의 run() 메서드를 실행.

  - ToolRunner.run(Cunfiguration conf, Tool tool, String[] args) 메서드를 이용해 Tool의 run() 메서드를 실행.



#DelayCount 만들기

- DelayCountMapper.java

```java
package lab.hadoop.delaycount;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class DelayCountMapper extends
		Mapper<LongWritable, Text, Text, IntWritable> {
	
	//job distinct
	private String workType;
	
	//map output value
	private final static IntWritable outputValue = new IntWritable(1);
	
	//map output key
	private Text outputKey = new Text();

	@Override
	protected void setup(Context context)
			throws IOException, InterruptedException {
		workType = context.getConfiguration().get("workType");
	}

	@Override
	protected void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {
		
		if (key.get()>0) {
			//comma distinction part
			String[] colums = value.toString().split(",");
			if (colums != null && colums.length > 0) {
				try {
					//Departure Delay data output
					if (workType.equals("departure")) {
						if(!colums[15].equals("NA")) {
							int depDelayTime = Integer.parseInt(colums[15]);
							if (depDelayTime > 0) {
								//output key setting
								outputKey.set(colums[0] + "," + colums[1]);
								
								//output data creation
								context.write(outputKey, outputValue);
							}
						}
					} else if (workType.equals("arrival")) {
						if(!colums[14].equals("NA")) {
							int arrDelayTime = Integer.parseInt(colums[14]);
							if (arrDelayTime > 0) {
								//output key setting
								outputKey.set(colums[0] + "," + colums[1]);
								
								//output data creation
								context.write(outputKey, outputValue);
							}
						}
					}
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		}
	}
}
```



- DelayCountReducer.java

```java
package lab.hadoop.delaycount;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class DelayCountReducer extends
		Reducer<Text, IntWritable, Text, IntWritable>{
	
	private IntWritable result = new IntWritable();
	
	//Total Departure Delay Time by Month
	public void reduce(Text key, Iterable<IntWritable> values, Context context) 
            throws IOException, InterruptedException {
		int sum = 0;
		for (IntWritable value : values)
		sum += value.get();
		result.set(sum);
		context.write(key, result);
		}
}
```



- DelayCount.java

```java
package lab.hadoop.delaycount;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;


public class DelayCount extends Configured implements Tool{
	
	public int run(String[] args) throws Exception {
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
		
		if (otherArgs.length != 2) {
			System.err.println("Usage: DelayCount <in> <out>");
			System.exit(2);
		}
		
		Job job = new Job(getConf(), "DelayCount");
		
		FileSystem hdfs = FileSystem.get(getConf());
		
		Path path = new Path(args[1]);
		if (hdfs.exists(path)) {
			hdfs.delete(path, true);
		}
		
		FileInputFormat.addInputPath(job,  new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		
		job.setJarByClass(DelayCount.class); //set main class 
	    job.setMapperClass(DelayCountMapper.class); //set mapper class
	    job.setReducerClass(DelayCountReducer.class); //set reducer class
	      
	    job.setInputFormatClass(TextInputFormat.class); // set input format 
	    job.setOutputFormatClass(TextOutputFormat.class); //set output format
	    
	    //output key, value type 
	    job.setOutputKeyClass(Text.class);
	    job.setOutputValueClass(IntWritable.class);
	   
	    job.waitForCompletion(true);
	    return 0;
	}
	
	public static void main(String[] args) throws Exception {
		//Tool interface execution
		int res = ToolRunner.run(new Configuration(), new DelayCount(), args);
		System.out.println("## Result: " + res);
	}
}
```



- [hadoop@master ~]$ hadoop fs -mkdir /output/delaycount => 산출값을 받을 delaycount폴더 생성
- [hadoop@master ~]$ hadoop jar ./delaycount.jar -D workType=departure /data/airline /output/delaycount  => jar의 코드 실행해서 산출
- [hadoop@master ~]$ hadoop fs -cat /output/delaycount/part-r-00000 => 산출결과 확인





-------------

**#정렬(Sorting)**

- 맵리듀스의 핵심 기능

- 하나의 리듀스 태스크만 실행되게 하면 정렬을 쉽게 해결할 수 있지만, 여러 데이터 노드가 구성된 상황에서 하나의 리듀스 태스크만 실행하는 것은 분산 환경의 장점을 살리지 않는 것이다.

- 대량의 데이터를 정렬하게 될 경우 네트워크 부하도 상당함

- 하둡이 제공하는 정렬 방식
  - 보조 정렬(Secondary Sort)
  - 부분 정렬(Partial Sort)
  - 전체 정렬(Total Sort)



**#보조 정렬(Secondary Sort)**

- 키의 값들을 그룹핑하고, 그룹핑된 레코드에 순서를 부여하는 방식

- 구현 순서
  -  기존 키의 값들을 조합한 복합키(Composite Key) 정의

-  키의 값 중에서 그룹핑 키로 사용할 키 결정
  -  복합키의 레코드를 정렬하기 위한 비교기(Comparator) 정의
  -  그룹핑 키를 파티셔닝할 파티셔너(Partitioner) 정의
  - 그룹핑 키를 정렬하기 위한 비교기(Comparator) 정의



#실습

- DateKey.java

```java
package lab.hadoop.sort;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableUtils;

public class DateKey implements WritableComparable<DateKey> {
	
	private String year;
	private Integer month;
	
	
	public DateKey() {
	}


	public DateKey(String year, Integer month) {
		super();
		this.year = year;
		this.month = month;
	}


	public String getYear() {
		return year;
	}


	public void setYear(String year) {
		this.year = year;
	}


	public Integer getMonth() {
		return month;
	}


	public void setMonth(Integer month) {
		this.month = month;
	}


	@Override
	public String toString() {
		return "DateKey [year=" + year + ", month=" + month + "]";
	}


	@Override
	public void readFields(DataInput in) throws IOException {
		year = WritableUtils.readString(in);
		month = in.readInt();
	}


	@Override
	public void write(DataOutput out) throws IOException {
		WritableUtils.writeString(out, year);
		out.writeInt(month);
	}


	@Override
	public int compareTo(DateKey key) {
		int result = year.compareTo(key.year);
		if (0 == result) {
			result = month.compareTo(key.month);
		}
		return 0;
	}	
}
```



- DateKeyComparator.java

```java
package lab.hadoop.sort;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

public class DateKeyComparator extends WritableComparator {
	protected DateKeyComparator() {
		super(DateKey.class, true);
	}

	@SuppressWarnings("rawtypes")
	@Override
	public int compare(WritableComparable w1, WritableComparable w2) {
		DateKey k1 = (DateKey) w1;
		DateKey k2 = (DateKey) w2;
		
		//compare year
		int cmp = k1.getYear().compareTo(k2.getYear());
		if (cmp != 0) {
			return cmp;
		}
		
		//compare month
		return k1.getMonth()==k2.getMonth() ? 0:(k1.getMonth()<k2.getMonth()?-1:1);
	}
}
```



- GroupKeyComparator.java

```java
package lab.hadoop.sort;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

public class GroupKeyComparator extends WritableComparator {
	
	protected GroupKeyComparator() {
		super(DateKey.class, true);
	}

	@SuppressWarnings("rawtypes")
	@Override
	public int compare(WritableComparable w1, WritableComparable w2) {
		DateKey k1 = (DateKey) w1;
		DateKey k2 = (DateKey) w2;
		return k1.getYear().compareTo(k2.getYear());
	}
}
```



- GroupKeyPartitioner.java

```java
package lab.hadoop.sort;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Partitioner;

public class GroupKeyPartitioner
			extends Partitioner<DateKey, IntWritable>{

	@Override
	public int getPartition(DateKey key, IntWritable val, int numPartitions) {
		int hash = key.getYear().hashCode();
		int partition = hash % numPartitions;
		return partition;
	}
}
```



- DelayCounters.enum

```java
package lab.hadoop.sort;

public enum DelayCounters {
	not_available_arrival,
	scheduled_arrival,
	early_arrival,
	not_available_departure,
	scheduled_departure,
	early_departure;
}
```



- DelayCountWithDateKeyMapper

```java
package lab.hadoop.sort;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class DelayCountMapperWithDateKey extends
		Mapper<LongWritable, Text, DateKey, IntWritable> {
	
	//map output value
	private final static IntWritable outputValue = new IntWritable(1);
	
	//map output key
	private DateKey outputKey = new DateKey();

	@Override
	public void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {
		
		if (key.get() > 0) {
			//insert comma distributor
			String[] colums = value.toString().split(",");
			if (colums != null && colums.length > 0) {
				try {
					
					//output departure delay data
					if (!colums[15].equals("NA")) {
						int depDelayTime = Integer.parseInt(colums[15]);
						if (depDelayTime > 0) {
							//set output key
							outputKey.setYear("D," + colums[0]);
							outputKey.setMonth(new Integer(colums[1]));
							
							//create output data
							context.write(outputKey, outputValue);
						} else if (depDelayTime == 0) {
							context.getCounter(DelayCounters.scheduled_departure).increment(1);
						} else if (depDelayTime < 0) {
							context.getCounter(DelayCounters.early_departure).increment(1);
						}
					} else {
						context.getCounter(DelayCounters.not_available_departure).increment(1);
					}
					
					
					//output arrival delay data
					if (!colums[14].equals("NA")) {
						int arrDelayTime = Integer.parseInt(colums[14]);
						if (arrDelayTime > 0) {
							//set output key
							outputKey.setYear("A," + colums[0]);
							outputKey.setMonth(new Integer(colums[1]));
							
							//create output data
							context.write(outputKey, outputValue);
						} else if (arrDelayTime == 0) {
							context.getCounter(DelayCounters.scheduled_arrival).increment(1);
						} else if (arrDelayTime < 0) {
							context.getCounter(DelayCounters.early_arrival).increment(1);
						}
					} else {
						context.getCounter(DelayCounters.not_available_arrival).increment(1);
					}
					
					
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		}
		
	}	
}
```



- DelayCountReducerWithDateKey

```java
package lab.hadoop.sort;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;

public class DelayCountReducerWithDateKey extends 
		Reducer<DateKey, IntWritable, DateKey, IntWritable> {
	
	private MultipleOutputs<DateKey, IntWritable> mos;
	
	//reduce output key
	private DateKey outputKey = new DateKey();
	
	//reduce output value
	private IntWritable result = new IntWritable();

	@Override
	protected void setup(Context context)
			throws IOException, InterruptedException {
		mos = new MultipleOutputs<DateKey, IntWritable>(context);
	}

	@Override
	protected void reduce(DateKey key, Iterable<IntWritable> values,
			Context context) throws IOException, InterruptedException {
		//comma distributer
		String[] colums = key.getYear().split(",");
		
		int sum = 0;
		Integer bMonth = key.getMonth();
		
		if (colums[0].equals("D")) {
			for (IntWritable value : values) {
				if (bMonth != key.getMonth()) {
					result.set(sum);
					outputKey.setYear(key.getYear().substring(2));
					outputKey.setMonth(bMonth);
					mos.write("departure", outputKey, result);
					sum = 0;
				}
				sum += value.get();
				bMonth = key.getMonth();
			}
			if (key.getMonth() == bMonth) {
				outputKey.setYear(key.getYear().substring(2));
				outputKey.setMonth(key.getMonth());
				result.set(sum);
				mos.write("departure", outputKey, result);
			}
		} else {
			for (IntWritable value : values) {
				if (bMonth != key.getMonth()) {
					result.set(sum);
					outputKey.setYear(key.getYear().substring(2));
					outputKey.setMonth(bMonth);
					mos.write("arrival", outputKey, result);
					sum = 0;
				}
				sum += value.get();
				bMonth = key.getMonth();
			}
			if (key.getMonth() == bMonth) {
				outputKey.setYear(key.getYear().substring(2));
				outputKey.setMonth(key.getMonth());
				result.set(sum);
				mos.write("arrival", outputKey, result);
			}
		}		
	}

	@Override
	protected void cleanup(Context context)
			throws IOException, InterruptedException {
		mos.close();
	}
}
```



- DelayCountWithDate

```java
package lab.hadoop.sort;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;


public class DelayCountWithDateKey extends Configured implements Tool{
	
	public int run(String[] args) throws Exception {
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
		
		if (otherArgs.length != 2) {
			System.err.println("Usage: DelayCountWithDateKey <in> <out>");
			System.exit(2);
		}
		
		@SuppressWarnings("deprecation")
		Job job = new Job(getConf(), "DelayCountWithDateKey");
		
		FileSystem hdfs = FileSystem.get(getConf());
		
		Path path = new Path(args[1]);
		if (hdfs.exists(path)) {
			hdfs.delete(path, true);
		}
		
		FileInputFormat.addInputPath(job,  new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		
		job.setJarByClass(DelayCountWithDateKey.class); //set main class 
		job.setPartitionerClass(GroupKeyPartitioner.class);
		job.setGroupingComparatorClass(GroupKeyComparator.class);
		job.setSortComparatorClass(DateKeyComparator.class);
		
	    job.setMapperClass(DelayCountMapperWithDateKey.class); //set mapper class
	    job.setReducerClass(DelayCountReducerWithDateKey.class); //set reducer class
	      
	    //output key, value type 
	    job.setOutputKeyClass(DateKey.class);
	    job.setOutputValueClass(IntWritable.class);
	    
	    job.setInputFormatClass(TextInputFormat.class); // set input format 
	    job.setOutputFormatClass(TextOutputFormat.class); //set output format
	    
	    //set MultipleOutputs
	    MultipleOutputs.addNamedOutput(job, "departure",
	    		TextOutputFormat.class, DateKey.class, IntWritable.class);
	    MultipleOutputs.addNamedOutput(job, "arrival",
	    		TextOutputFormat.class, DateKey.class, IntWritable.class);
	    
	    job.waitForCompletion(true);
	    return 0;
	}
	
	public static void main(String[] args) throws Exception {
		//Tool interface execution
		int res = ToolRunner.run(new Configuration(), new DelayCountWithDateKey(), args);
		System.out.println("## Result: " + res);
	}
}
```



- hadoop fs -mkdir /output/groupkey => 산출 데이터 받을 폴더 생성
- hadoop jar groupkey.jar /data/airline/ /output/groupkey/ => Export한 jar파일 돌려서 데이터 산출

- hadoop fs -ls 

```
Found 4 items
-rw-r--r--   1 hadoop supergroup          0 2019-08-20 01:28 /output/groupkey/_SUCCESS
-rw-r--r--   1 hadoop supergroup        435 2019-08-20 01:28 /output/groupkey/arrival-r-00000
-rw-r--r--   1 hadoop supergroup        435 2019-08-20 01:28 /output/groupkey/departure-r-00000
-rw-r--r--   1 hadoop supergroup          0 2019-08-20 01:28 /output/groupkey/part-r-00000
```



- hadoop fs -cat /output/groupkey/arrival-r-00000

```
DateKey [year=2007, month=1]	286334
DateKey [year=2007, month=2]	284152
DateKey [year=2007, month=3]	293360
DateKey [year=2007, month=4]	273055
DateKey [year=2007, month=5]	275332
DateKey [year=2007, month=6]	326446
DateKey [year=2007, month=7]	326559
DateKey [year=2007, month=8]	317197
DateKey [year=2007, month=9]	225751
DateKey [year=2007, month=10]	270098
DateKey [year=2007, month=11]	242722
DateKey [year=2007, month=12]	332449
DateKey [year=2008, month=1]	279427
DateKey [year=2008, month=2]	278902
DateKey [year=2008, month=3]	294556
DateKey [year=2008, month=4]	256142
DateKey [year=2008, month=5]	254673
DateKey [year=2008, month=6]	295897
DateKey [year=2008, month=7]	264630
DateKey [year=2008, month=8]	239737
DateKey [year=2008, month=9]	169959
DateKey [year=2008, month=10]	183582
DateKey [year=2008, month=11]	181506
DateKey [year=2008, month=12]	280493
```



- hadoop fs -cat /output/groupkey/departure-r-00000

```
DateKey [year=2007, month=1]	255777
DateKey [year=2007, month=2]	259288
DateKey [year=2007, month=3]	276261
DateKey [year=2007, month=4]	249097
DateKey [year=2007, month=5]	241699
DateKey [year=2007, month=6]	307986
DateKey [year=2007, month=7]	307864
DateKey [year=2007, month=8]	298530
DateKey [year=2007, month=9]	195615
DateKey [year=2007, month=10]	231129
DateKey [year=2007, month=11]	217557
DateKey [year=2007, month=12]	304011
DateKey [year=2008, month=1]	247948
DateKey [year=2008, month=2]	252765
DateKey [year=2008, month=3]	271969
DateKey [year=2008, month=4]	220864
DateKey [year=2008, month=5]	220614
DateKey [year=2008, month=6]	271014
DateKey [year=2008, month=7]	253632
DateKey [year=2008, month=8]	231349
DateKey [year=2008, month=9]	147061
DateKey [year=2008, month=10]	162531
DateKey [year=2008, month=11]	157278
DateKey [year=2008, month=12]	263949
```

=> 연도별로, 월별로 정렬돼서 나온다!!



