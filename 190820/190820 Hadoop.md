# 190820 Hadoop

#맵 사이드 조인

-  setup 메서드에서 조인될 데이터를 준비합니다.
  -  Hashtable을 전역변수로 선언하고 Hashtable에 데이터를 저장함

  -  읽어들일 데이터가 분산캐시에 등록되어 있어야 함

    ​    파일의 유형에 따라 등록하는 파일이 다름

- map 메서드에서 write할 때 Hashtable의 값을 키로 저장합니다.

- 실행하기 전에 분산캐시로 사용할 파일을 HDFS 에 업로드 해야 함



#맵사이드 Join 실습

- http://stat-computing.org/dataexpo/2009/supplemental-data.html
  - carriers.csv download
- data/metadata 폴더 생성해서 carriers.csv 넣기
  - hadoop fs -mkdir data/metadata
  - hadoop fs -put ./Downloads/carriers.csv /data/metadata



- MapperWithMapSideJoin.java

```java
package lab.hadoop.join;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.Hashtable;

import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MapperWithMapSideJoin extends
		Mapper<LongWritable, Text, Text, Text> {

	private Hashtable<String, String> joinMap 
	                     = new Hashtable<String, String>();

	// map 출력키
	private Text outputKey = new Text();

	@Override
	public void setup(Context context) throws IOException, 
	                                       InterruptedException {
		try {
			// 분산캐시 조회
			Path[] cacheFiles = DistributedCache.getLocalCacheFiles(context
					.getConfiguration());
			// 조인 데이터 생성
			if (cacheFiles != null && cacheFiles.length > 0) {
				String line;
				String[] tokens;
				BufferedReader br = new BufferedReader(new FileReader(
						cacheFiles[0].toString()));
				try {
					while ((line = br.readLine()) != null) {
						tokens = line.toString().replaceAll("\"", "")
								.split(",");
						joinMap.put(tokens[0], tokens[1]);
					}
				} finally {
					br.close();
				}
			} else {
				System.out.println("### cache files is null!");
			}
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	public void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {

		if (key.get() > 0) {
			// 콤마 구분자 분리
			String[] colums = value.toString().split(",");
			if (colums != null && colums.length > 0) {
				try {
					outputKey.set(joinMap.get(colums[8]));
					context.write(outputKey, value);
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		}
	}
}
```



- MapsideJoin.java

```java
package lab.hadoop.join;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class MapsideJoin extends Configured implements Tool {

	public int run(String[] args) throws Exception {
		String[] otherArgs = new GenericOptionsParser(getConf(), args)
				.getRemainingArgs();
		// 입력출 데이터 경로 확인
		if (otherArgs.length != 3) {
			System.err.println("Usage: MapsideJoin <metadata> <in> <out>");
			System.exit(2);
		}
		
		Configuration conf = new Configuration();

		// 파일 시스템 제어 객체 생성
		FileSystem hdfs = FileSystem.get(conf);
		// 경로 체크
		Path path = new Path(args[2]);
		if (hdfs.exists(path)) {
			hdfs.delete(path, true);
		}
		
		// Job 이름 설정
		Job job = new Job(getConf(), "MapsideJoin");

		// 분산 캐시 설정
		DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(),
				job.getConfiguration());

		// 입출력 데이터 경로 설정
		FileInputFormat.addInputPath(job, new Path(otherArgs[1]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));

		// Job 클래스 설정
		job.setJarByClass(MapsideJoin.class);
		// Mapper 설정
		job.setMapperClass(MapperWithMapSideJoin.class);
		// Reducer 설정
		job.setNumReduceTasks(0);

		// 입출력 데이터 포맷 설정
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		// 출력키 및 출력값 유형 설정
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);

		job.waitForCompletion(true);
		return 0;
	}

	public static void main(String[] args) throws Exception {
		// Tool 인터페이스 실행
		int res = ToolRunner.run(new Configuration(), new MapsideJoin(), args);
		System.out.println("## RESULT:" + res);
	}
}
```



- jar파일 Export 후 다음 코드 실행

  - hadoop fs -mkdir /output/mapjoin  => output폴더에 데이터 산출할 mapjoin폴더 생성
  - hadoop jar ./mapsidejoin.jar /data/metadata/carriers.csv /data/airline /output/mapjoin  => 데이터 산출
  - 결과

  ```
  Found 13 items
  -rw-r--r--   1 hadoop supergroup          0 2019-08-20 23:43 /output/mapjoin/_SUCCESS
  -rw-r--r--   1 hadoop supergroup  171793742 2019-08-20 23:41 /output/mapjoin/part-m-00000
  -rw-r--r--   1 hadoop supergroup  175222135 2019-08-20 23:41 /output/mapjoin/part-m-00001
  -rw-r--r--   1 hadoop supergroup  172331002 2019-08-20 23:40 /output/mapjoin/part-m-00002
  -rw-r--r--   1 hadoop supergroup  171978436 2019-08-20 23:41 /output/mapjoin/part-m-00003
  -rw-r--r--   1 hadoop supergroup  172496788 2019-08-20 23:41 /output/mapjoin/part-m-00004
  -rw-r--r--   1 hadoop supergroup  170330756 2019-08-20 23:41 /output/mapjoin/part-m-00005
  -rw-r--r--   1 hadoop supergroup  173007600 2019-08-20 23:42 /output/mapjoin/part-m-00006
  -rw-r--r--   1 hadoop supergroup  171153506 2019-08-20 23:42 /output/mapjoin/part-m-00007
  -rw-r--r--   1 hadoop supergroup  170223990 2019-08-20 23:42 /output/mapjoin/part-m-00008
  -rw-r--r--   1 hadoop supergroup  172067905 2019-08-20 23:43 /output/mapjoin/part-m-00009
  -rw-r--r--   1 hadoop supergroup   41070240 2019-08-20 23:42 /output/mapjoin/part-m-00010
  -rw-r--r--   1 hadoop supergroup   22655731 2019-08-20 23:42 /output/mapjoin/part-m-00011
  ```

  

  - hadoop fs -tail /output/mapjoin/part-m-00001 => 다음과 같이 데이터 확인

  ```
  01,-11,-6,ORD,SWF,714,4,10,0,,0,0,0,0,0,0
  American Eagle Airlines Inc.	2007,5,15,2,941,945,1230,1245,MQ,4021,N902BC,109,120,94,-15,-4,ORD,SWF,714,4,11,0,,0,0,0,0,0,0
  American Eagle Airlines Inc.	2007,5,16,3,1005,945,1252,1245,MQ,4021,N616AE,107,120,93,7,20,ORD,SWF,714,5,9,0,,0,0,0,0,0,0
  American Eagle Airlines Inc.	2007,5,17,4,943,945,1251,1245,MQ,4021,N682AE,128,120,97,6,-2,ORD,SWF,714,4,27,0,,0,0,0,0,0,0
  American Eagle Airlines Inc.	2007,5,18,5,942,945,1241,1245,MQ,4021,N667GB,119,120,106,-4,-3,ORD,SWF,714,3,10,0,,0,0,0,0,0,0
  American Eagle Airlines Inc.	2007,5,19,6,940,945,1241,1245,MQ,4021,N648AE,121,120,105,-4,-5,ORD,SWF,714,4,12,0,,0,0,0,0,0,0
  American Eagle Airlines Inc.	2007,5,20,7,944,945,1255,1245,MQ,4021,N660CL,131,120,99,10,-1,ORD,SWF,714,5,27,0,,0,0,0,0,0,0
  American Eagle Airlines Inc.	2007,5,21,1,950,945,1252,1245,MQ,4021,N644AE,122,120,95,7,5,ORD,SWF,714,4,23,0,,0,0,0,0,0,0
  American Eagle Airlines Inc.	2007,5,23,3,950,945,1253,1245,MQ,4021,N669MB,123,120,105,8,5,ORD,SWF,714,4,14,0,,0,0,0,0,0,0
  ```

  



**#리듀스 사이드 조인**

- 두 개의 데이터를 키/값으로 출력.
- 조인될 키를 구분하기 위해 키 뒤에 임의의 문자 추가해서 출력
- ex) WN_A(항공운항통계 데이터의 항공사 코드) , WN_B(항공사 코드 데이터의 항공사 코드)
- 리듀스에서 출력 시 추가된 문자열에 따라 다른 키의 값을 키로 저장
- _A가 붙어있으면 키를 WN_B의 값으로 저장



#리듀스 사이드 조인 실습

- CarrierCodeMapper.java

```java
package lab.hadoop.join;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class CarrierCodeMapper extends
		Mapper<LongWritable, Text, Text, Text> {
	
	public final static String DATA_TAG = "A";
	
	private Text outputKey = new Text();
	private Text outputValue = new Text();
	
	@Override
	protected void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {
		if (key.get()>0) {
			String[] colums = value.toString().replaceAll("\"", "").split(",");
			if (colums != null && colums.length > 0) {
				outputKey.set(colums[0] + "_" + DATA_TAG);
				outputValue.set(colums[1]);
				context.write(outputKey,  outputValue);
			}
		}
	}
}
```



- MapperWithReducesideJoin.java

```java
package lab.hadoop.join;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MapperWithReducesideJoin extends
		Mapper<LongWritable, Text, Text, Text> {
	
	public final static String DATA_TAG = "B";
	
	private Text outputKey = new Text();
	
	@Override
	protected void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {
		if (key.get()>0) {
			String[] colums = value.toString().split(",");
			if (colums != null && colums.length > 0) {
				try {
					outputKey.set(colums[8] + "_" + DATA_TAG);
					context.write(outputKey,  value);
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		}
	}
}
```



- ReducerWithReducesideJoin.java

```java
package lab.hadoop.join;

import java.io.IOException;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class ReducerWithReducesideJoin extends
		Reducer<Text, Text, Text, Text>{
	
	// reduce output key
	private Text outputKey = new Text();
	private Text outputValue = new Text();
	
	@Override
	protected void reduce(Text key, Iterable<Text> values, Context context)
			throws IOException, InterruptedException {
		
		String tagValue = key.toString().split("_")[1];
		
		for (Text value : values) {
			//set output key
			if (tagValue.equals(CarrierCodeMapper.DATA_TAG)) {
				outputKey.set(value);
				
				//set output value and create output data
			} else if (tagValue.equals(MapperWithReducesideJoin.DATA_TAG)) {
				outputValue.set(value);
				context.write(outputKey, outputValue);
			}
		}
	}
}
```



- ReducesideJoin.java

```java
package lab.hadoop.join;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class ReducesideJoin extends Configured implements Tool {

   @Override
   public int run(String[] args) throws Exception {
      String[] otherArgs=new GenericOptionsParser(getConf(), args).getRemainingArgs();
      //check in-out data path
      if(otherArgs.length!=3) {
         System.err.println("Usage : ReducesideJoin <metadata> <in> <out>");
         System.exit(2);
      }//if end
      
      Configuration conf= new Configuration();
      
      FileSystem fs = FileSystem.get(conf);//create filesystem control reference
      
      //path check 
      Path path = new Path(args[2]);
      if(fs.exists(path)) fs.delete(path, true); //if end
      
      Job job = Job.getInstance(conf, "ReducesideJoin");//set job name
      
      //set out data path 
      FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));
      
      job.setJarByClass(ReducesideJoin.class); //set jar class
      job.setReducerClass(ReducerWithReducesideJoin.class); //set reducer class
      
      //set in-out data format
      job.setInputFormatClass(TextInputFormat.class);
      job.setOutputFormatClass(TextOutputFormat.class);
      
      //set output key, value type
      job.setOutputKeyClass(Text.class);
      job.setOutputValueClass(Text.class);
      
      //multipleinputs
      MultipleInputs.addInputPath(job, new Path(otherArgs[0]), TextInputFormat.class, CarrierCodeMapper.class);
      MultipleInputs.addInputPath(job, new Path(otherArgs[1]), TextInputFormat.class, MapperWithReducesideJoin.class);
      
      job.waitForCompletion(true);
            
      return 0;
   }

   public static void main(String[] args) throws Exception {
      int res = ToolRunner.run(new Configuration(), new ReducesideJoin(), args);
      System.out.println("## RESULT(ReducesideJoin) : " + res);

   }
}
```



- hadoop jar reducesidejoin.jar /data/metadata/carriers.csv /data/airline/ /output/reducesidejoin/ => jar파일 돌려서 데이터 산출
- hadoop fs -tail /output/reducesidejoin/part-r-00000 => 데이터 확인

```
,0
Mesa Airlines Inc.	2007,1,10,3,2028,2035,2205,2212,YV,7278,N27185,97,97,77,-7,-7,IAD,BOS,413,7,13,0,,0,0,0,0,0,0
Mesa Airlines Inc.	2007,1,10,3,1936,1935,2054,2112,YV,7220,N75995,78,97,61,-18,1,IAD,BOS,413,5,12,0,,0,0,0,0,0,0
Mesa Airlines Inc.	2007,1,10,3,1444,1445,1615,1614,YV,7208,N506MJ,91,89,69,1,-1,IAD,BOS,413,7,15,0,,0,0,0,0,0,0
Mesa Airlines Inc.	2007,1,10,3,1020,1016,1137,1145,YV,7204,N75995,77,89,61,-8,4,IAD,BOS,413,5,11,0,,0,0,0,0,0,0
Mesa Airlines Inc.	2007,1,10,3,1225,1230,1401,1359,YV,7160,N505MJ,96,89,71,2,-5,IAD,BOS,413,8,17,0,,0,0,0,0,0,0
Mesa Airlines Inc.	2007,1,10,3,911,917,1003,1006,YV,7355,N37342,112,109,91,-3,-6,IAD,BNA,542,3,18,0,,0,0,0,0,0,0
Mesa Airlines Inc.	2007,1,10,3,2135,2140,2230,2231,YV,7277,N856MJ,115,111,87,-1,-5,IAD,BNA,542,8,20,0,,0,0,0,0,0,0
Mesa Airlines Inc.	2007,1,10,3,1308,1310,1400,1405,YV,7174,N17217,112,115,90,-5,-2,IAD,BNA,542,9,13,0,,0,0,0,0,0,0
Mesa Airlines Inc.	2007,1,10,3,1740,1640,1830,1735,YV,7149,N858MJ,110,115,88,55,60,IAD,BNA,542,5,17,0,,0,55,0,0,0,0
```





------------------------

## Hive

**#MapReduce를 직관적 “SQL“기반의 하둡 에코 시스템중 하나인 “Hive“를 통해서 실행 하는 경우**

1. 다소 복잡한 “MR 프로그래밍”이 보다 친근하고, 직관적인 “SQL”의 지원

2. 다이나믹한 검색 조건 지정

3. 매번 “Name Node” 배포 없이 원격(Remote)에서 “MR Job”를 실행 지원



- Hive 내장 모드
  - 설정 변경을 하지 않는 기본 구성. DBMS로  Derby를 이용한다.
  - 혼자서 테스트 용도로 사용하기에 적합한 구성

- Hive 로컬 모드
  - Hive 클라이언트와 메타스토어로부터 DBMS를 독립시키는 구성이다.
  - DBMS는 JDBC를 통해 접속한다. 
  - 로컬 모드에서는 다수의 접속을 동시에 허용하지만, Hive 클라이언트가 모드 같은 노드에 존재해야 한다. 

- Hive 원격 모드
  - DBMS뿐만 아니라 메타스토어도 독립시킨 구성이다.
  - Hive 클라이언트가 Thrift API를 경유해서 원격으로 메타스토어에 접속할 수 있다



- HiveQL이라 불리는 SQL 유사 언어를 이용해서 MapReduce를 실행하는 것

- 페이스북 멤버를 중심으로 개발이 진행되고 있다

- HiveQL이 취급하는 데이터는 논리적 행과 열로 이루어진 테이블 구조로, HDFS상에 파일로 존재한다

- HiveQL로 기술한 쿼리는 MapReduce 같은 일련의 처리로 변환되어 테이블을 조작한다.

- 컴파일 없이 바로 실행할 수 있으므로, Ad-hoc 처리에 적합

- Hive는 테이블 정의 등의 정보를 Metastore로 관리하고 있으며, 테이블 Meta정보를 저장하기 위해 RDBMS가 필요하다.



- Hive(Hadoop)과 RDBMS의 차이
  - 온라인 처리에 부적합 (MapReduce의 잡 하나를 실행하면 아무것도 하지 않아도 오버헤드로 20~30초 정도 시간이 걸리므로…..일괄처리를 고속으로 실행하기 위한 것)
  - 인덱스 및 트랜잭션 기능이 없다 

  - 롤백(rollback)처리가 없다. (복수의 HiveQL을 병렬 실행하여 그중 하나라도 실패하면, 사용자 스스로 잡 관리와 불필요한 처리 결과를 삭제해주어야 한다)

  - MapReduce의 keep.failed.task.files 파라미터는 MapReduce 잡이 실패하면 MapReduce 프레임워크 중간 파일은 종료시에 삭제되도록 초기 설정이 되어 있다
  - Hive에는 Update나 Delete문이 없다

- Hive 데이터는 HDFS 상의 파일로 존재하며, Hive 테이블은 HDFS 디렉토리로 존재한다

- Hive 데이터베이스나 스키마도 HDFS 상의 디렉터리로 존재한다. (/user/hive/warehouse/테이블명  디렉터리로 존재)

- Hive 에서 컬럼이나 속성 등 테이블 실체가 아닌, 속성 정보에 해당하는 테이블 정의는 metastore라 불리며, RDBMS에 저장된다.

- Hive의 테이블 정의에서는 파티션이라 불리는 물리적 관리 단위를 지정할 수 있다

- 파티션은 HDFS 상의 디렉토리를 분할하는 것과 같다

- 파티션을 설정함으로써 처리 범위를 제어할 수 있어, 처리 고속화가 가능하다.

- 파티션 내의 모든 데이터가 필요 없어지면, 파티션 단위로 삭제할 수 있어서 관리도 수월하다

- HiveQL의 흐름은 Hive에서 쿼리문 앞에 EXPLAIN을 붙여 실행하면 확인할 수 있다

- HiveSQ은 Stage라는 단위로 MapReduce나 부속 처리로 변환되어, Stage 간 의존 관계가 생성된다





\#HiveQL이 기술되어 있는 파일을 인수로 사용하여 명령어 라인에서 실행 

- hive  -f  <파일명>     



#명령어 라인 인수로 HiveQL문을 직접 기술해서 실행

- hive  -e  ‘HiveSQL문’



#Hive는 실행 시에 Hive에 관한 로그나 MapREduce 잡 실행 상태에 대한 정보를 출력하지만, -s 인수를 지정하면 silent 모드가 돼서 출력을 억제하는 것이 가능

- hive  -s  -e  ‘HiveSQL문’    

 

 #Hive 명령을 실행함으로써 Hive 쉘을 시작할 수 있다

#SET 명령을 통해 Hadoop이나 Hive 관련 속성을 설정 할 수 있다.  (설정한 것은 Hive 쉘 내에서만 유효하다) 

- hive> SET mapred.reduce.tasks= 20

- hive> SET  hive.parallel.queries=true;



**#Hive 설치**

1. http://apache.tt.co.kr/hive/hive-1.2.2/ 에서 tar파일 다운로드

2. 압축 풀고 권한 부여

   ```
   [root@master local]# tar -xzvf /home/hadoop/Downloads/apache-hive-1.2.2-bin.tar.gz 
   [root@master local]# chown -R hadoop:hadoop apache-hive-1.2.2-bin/
   ```

3. 이름 바꾸기

   ```
   [root@master local]# ln -s apache-hive-1.2.2-bin/  hive
   [root@master local]# ls -l
   
   [root@master local]# chown -R hadoop:hadoop hive
   [root@master local]# ls -l
   ```

4. 마스터에서 hadoop 환경설정 파일 변경

   ```
   [root@master local]# su - hadoop
   [hadoop@master ~]$ vi .bash_profile
   
   export HIVE_HOME=/usr/local/hive
   export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:
   ```

5. 마스터 노드에 hive 메타스토어 mysql 구성 (로컬모드)

   ```
   [root@master ~]# rpm -ivh /home/hadoop/mysql/mysql-community-release-el6-5.noarch.rpm
   [root@master ~]#  ls -la /etc/yum.repos.d/ => 설치가 잘 됐는지 확인
   [root@master ~]# yum install mysql-server => 중간에 전부 Yes
   
   [root@master ~]# ls /usr/bin/mysql
   [root@master ~]# ls /usr/sbin/mysqld
   [root@master ~]#  service mysqld start
   ```

6. mysql 서버 시작

   ```
   [root@master local]# service mysqld start  => 서비스 시작 후
   [root@master ~]# mysql --version  => 버젼 확인
   [root@master ~]# netstat -anp | grep mysql =>포트번호 확인
   
   [root@master ~]# usr/bin/mysql => mysql 실행
   ```

7. 루트 사용자의 암호를 설정한다.

   ```
   mysql> grant all privileges on *.* to hive@localhost identified by 'hive' with grant option ;
   mysql> flush privileges;
   mysql> show databases;
   mysql> use mysql
   mysql> show tables;
   mysql> select user from user; => hive 유저가 잘 생성됐나 확인
   ```

8.  hive-env.sh  설정파일 생성 및 변경

   ```
   [hadoop@master ~]$ cd /usr/local/hive/conf/
   [hadoop@master ~]$ cp hive-env.sh.template  hive-env.sh
   [hadoop@master ~]$ vi hive-env.sh
   HADOOP_HOME=/usr/local/hadoop-2.7.7 (주석 없애고 다음과 같이 수정)
   [hadoop@master ~]$  chmod 755 hive-env.sh 
   ```

9.  /usr/local/hive/conf/hive-site.xml을 수정

   ```
   [hadoop@master ~]$ vi /usr/local/hive/conf/hive-site.xml => 들어가서 아래와 같이 수정
   
   <?xml version="1.0"?>
   <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
   <configuration>
   <property>
     <name>hive.metastore.local</name>
     <value>true</value>
   </property>
   <property>
     <name>javax.jdo.option.ConnectionURL</name>
     <value>jdbc:mysql://localhost:3306/metastore_db?createDatabaseIfNotExist=true</value>
     <description>JDBC connect string for a JDBC metastore</description>
   </property>
   <property>
     <name>javax.jdo.option.ConnectionDriverName</name>
     <value>com.mysql.jdbc.Driver</value>
     <description>Driver class name for a JDBC metastore</description>
   </property>
   <property>
     <name>javax.jdo.option.ConnectionUserName</name>
     <value>hive</value>
     <description>username to use against metastore database</description>
   </property>
   
   <property>
     <name>javax.jdo.option.ConnectionPassword</name>
     <value>hive</value>
     <description>password to use against metastore database</description>
   </property> 
     </configuration>
   ```

   

